{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the MLE-Infrastructure \ud83d\udd2c","text":"Experiment Logging Parameter Searches Experiment Launch Experiment Protocol Experiment Manager <code>mle-logging</code> <code>mle-hyperopt</code> <code>mle-scheduler</code> <code>mle-monitor</code> <code>mle-toolbox</code> <p>The MLE-Infrastructure provides a reproducible workflow for distributed Machine Learning experimentation (MLE) with minimal engineering overhead. The core consists of 5 packages:</p> <ul> <li><code>mle-logging</code>: Experiment logging with easy multi-seed and configuration aggregation.</li> <li><code>mle-hyperopt</code>: Hyperparameter Optimization with config export, refinement &amp; reloading.</li> <li><code>mle-monitor</code>: Monitor cluster/cloud VM resource utilization &amp; protocol experiments.</li> <li><code>mle-scheduler</code>: Schedule &amp; monitor jobs on Slurm, GridEngine clusters &amp; GCP VMs.</li> <li><code>mle-toolbox</code>: Glues everything together to manage &amp; post-process experiments.</li> </ul> <p> </p> <p>Note I: A template repository of an infrastructure-based project can be found in the <code>mle-project</code>. You can inspect your experiment stack in an interactive web UI: <code>mle-laboratory</code>.</p> <p>Note II: <code>mle-logging</code>, <code>mle-hyperopt</code>, <code>mle-monitor</code> and <code>mle-scheduler</code> are standalone packages and can be used independently of the utilities provided by the <code>mle-toolbox</code>.</p>"},{"location":"mle_hyperopt/","title":"The <code>mle-hyperopt</code> Package","text":""},{"location":"mle_hyperopt/#hyperparameter-optimization-made-easy","title":"Hyperparameter optimization made easy \ud83d\ude80","text":"<p>The <code>mle-hyperopt</code> package provides a simple and intuitive API for hyperparameter optimization of your Machine Learning Experiment (MLE) pipeline. It supports real, integer &amp; categorical search variables and single- or multi-objective optimization.</p> <p>Core features include the following:</p> <ul> <li>API Simplicity: <code>strategy.ask()</code>, <code>strategy.tell()</code> interface &amp; space definition.</li> <li>Strategy Diversity: Grid, random, coordinate search, SMBO &amp; wrapping FAIR's <code>nevergrad</code>, Successive Halving, Hyperband, Population-Based Training.</li> <li>Search Space Refinement based on the top performing configs via <code>strategy.refine(top_k=10)</code>.</li> <li>Export of configurations to execute via e.g. <code>python train.py --config_fname config.yaml</code>.</li> <li>Storage &amp; reload search logs via <code>strategy.save(&lt;log_fname&gt;)</code>,  <code>strategy.load(&lt;log_fname&gt;)</code>.</li> </ul> <p>For a quickstart check out the notebook blog \ud83d\udcd6.</p>"},{"location":"mle_hyperopt/#the-api","title":"The API \ud83c\udfae","text":"<pre><code>from mle_hyperopt import RandomSearch\n\n# Instantiate random search class\nstrategy = RandomSearch(real={\"lrate\": {\"begin\": 0.1,\n                                        \"end\": 0.5,\n                                        \"prior\": \"log-uniform\"}},\n                        integer={\"batch_size\": {\"begin\": 32,\n                                                \"end\": 128,\n                                                \"prior\": \"uniform\"}},\n                        categorical={\"arch\": [\"mlp\", \"cnn\"]})\n\n# Simple ask - eval - tell API\nconfigs = strategy.ask(5)\nvalues = [train_network(**c) for c in configs]\nstrategy.tell(configs, values)\n</code></pre>"},{"location":"mle_hyperopt/#implemented-search-types","title":"Implemented Search Types    \ud83d\udd2d","text":"Search Type Description <code>search_config</code> <code>GridSearch</code> Search over list of discrete values - <code>RandomSearch</code> Random search over variable ranges <code>refine_after</code>, <code>refine_top_k</code> <code>CoordinateSearch</code> Coordinate-wise optimization with fixed defaults <code>order</code>, <code>defaults</code> <code>SMBOSearch</code> Sequential model-based optimization (Hutter et al., 2011) <code>base_estimator</code>, <code>acq_function</code>, <code>n_initial_points</code> <code>NevergradSearch</code> Multi-objective nevergrad wrapper <code>optimizer</code>, <code>budget_size</code>, <code>num_workers</code> <code>HalvingSearch</code> Successive Halving (Karmin et al., 2013) <code>min_budget</code>, <code>num_arms</code>, <code>halving_coeff</code> <code>HyperbandSearch</code> Hyperband (Li et al., 2018) <code>max_resource</code>, <code>eta</code> <code>PBTSearch</code> Population-Based Training (Jaderberg et al., 2017) <code>explore</code>, <code>exploit</code>"},{"location":"mle_hyperopt/#variable-types-hyperparameter-spaces","title":"Variable Types &amp; Hyperparameter Spaces \ud83c\udf0d","text":"Variable Type Space Specification <code>real</code> Real-valued <code>Dict</code>: <code>begin</code>, <code>end</code>, <code>prior</code>/<code>bins</code> (grid) <code>integer</code> Integer-valued <code>Dict</code>: <code>begin</code>, <code>end</code>, <code>prior</code>/<code>bins</code> (grid) <code>categorical</code> Categorical <code>List</code>: Values to search over"},{"location":"mle_hyperopt/#installation","title":"Installation \u23f3","text":"<p>A PyPI installation is available via:</p> <pre><code>pip install mle-hyperopt\n</code></pre> <p>Alternatively, you can clone this repository and afterwards 'manually' install it:</p> <pre><code>git clone https://github.com/mle-infrastructure/mle-hyperopt.git\ncd mle-hyperopt\npip install -e .\n</code></pre>"},{"location":"mle_hyperopt/#search-method-highlights","title":"Search Method Highlights \ud83d\udd0e","text":""},{"location":"mle_hyperopt/#grid-search","title":"Grid Search \ud83d\udfe5","text":"<pre><code>strategy = GridSearch(\n    real={\"lrate\": {\"begin\": 0.1,\n                    \"end\": 0.5,\n                    \"bins\": 5}},\n    integer={\"batch_size\": {\"begin\": 1,\n                            \"end\": 5,\n                            \"bins\": 1}},\n    categorical={\"arch\": [\"mlp\", \"cnn\"]},\n    fixed_params={\"momentum\": 0.9})\n\nconfigs = strategy.ask()\n</code></pre>"},{"location":"mle_hyperopt/#hyperband","title":"Hyperband \ud83c\udfb8","text":"<pre><code>strategy = HyperbandSearch(\n    real={\"lrate\": {\"begin\": 0.1,\n                    \"end\": 0.5,\n                    \"prior\": \"uniform\"}},\n    integer={\"batch_size\": {\"begin\": 1,\n                            \"end\": 5,\n                            \"prior\": \"log-uniform\"}},\n    categorical={\"arch\": [\"mlp\", \"cnn\"]},\n    search_config={\"max_resource\": 81,\n                   \"eta\": 3},\n    seed_id=42,\n    verbose=True)\n\nconfigs = strategy.ask()\n</code></pre>"},{"location":"mle_hyperopt/#population-based-training","title":"Population-Based Training \ud83e\udd8e","text":"<pre><code>strategy = PBTSearch(\n    real={\"lrate\": {\"begin\": 0.1,\n                    \"end\": 0.5,\n                    \"prior\": \"uniform\"}}\n    search_config={\n        \"exploit\": {\"strategy\": \"truncation\", \"selection_percent\": 0.2},\n        \"explore\": {\"strategy\": \"perturbation\", \"perturb_coeffs\": [0.8, 1.2]},\n        \"steps_until_ready\": 4,\n        \"num_workers\": 10,\n    },\n    maximize_objective=True\n)\n\nconfigs = strategy.ask()\n</code></pre>"},{"location":"mle_hyperopt/#further-options","title":"Further Options \ud83d\udeb4","text":""},{"location":"mle_hyperopt/#saving-reloading-logs","title":"Saving &amp; Reloading Logs \ud83c\udfea","text":"<pre><code># Storing &amp; reloading of results from .json/.yaml/.pkl\nstrategy.save(\"search_log.json\")\nstrategy = RandomSearch(..., reload_path=\"search_log.json\")\n\n# Or manually add info after class instantiation\nstrategy = RandomSearch(...)\nstrategy.load(\"search_log.json\")\n</code></pre>"},{"location":"mle_hyperopt/#search-decorator","title":"Search Decorator \ud83e\uddf6","text":"<pre><code>from mle_hyperopt import hyperopt\n\n@hyperopt(strategy_type=\"Grid\",\n          num_search_iters=25,\n          real={\"x\": {\"begin\": 0., \"end\": 0.5, \"bins\": 5},\n                \"y\": {\"begin\": 0, \"end\": 0.5, \"bins\": 5}})\ndef circle(config):\n    distance = abs((config[\"x\"] ** 2 + config[\"y\"] ** 2))\n    return distance\n\nstrategy = circle()\n</code></pre>"},{"location":"mle_hyperopt/#storing-configuration-files","title":"Storing Configuration Files \ud83d\udcd1","text":"<pre><code># Store 2 proposed configurations - eval_0.yaml, eval_1.yaml\nstrategy.ask(2, store=True)\n# Store with explicit configuration filenames - conf_0.yaml, conf_1.yaml\nstrategy.ask(2, store=True, config_fnames=[\"conf_0.yaml\", \"conf_1.yaml\"])\n</code></pre>"},{"location":"mle_hyperopt/#storing-checkpoint-paths","title":"Storing Checkpoint Paths \ud83d\udee5\ufe0f","text":"<pre><code># Ask for 5 configurations to evaluate and get their scores\nconfigs = strategy.ask(5)\nvalues = ...\n# Get list of checkpoint paths corresponding to config runs\nckpts = [f\"ckpt_{i}.pt\" for i in range(len(configs))]\n# `tell` parameter configs, eval scores &amp; ckpt paths\n# Required for Halving, Hyperband and PBT\nstrategy.tell(configs, scores, ckpts)\n</code></pre>"},{"location":"mle_hyperopt/#retrieving-top-performers-visualizing-results","title":"Retrieving Top Performers &amp; Visualizing Results \ud83d\udcc9","text":"<pre><code># Get the top k best performing configurations\nid, configs, values = strategy.get_best(top_k=4)\n\n# Plot timeseries of best performing score over search iterations\nstrategy.plot_best()\n\n# Print out ranking of best performers\nstrategy.print_ranking(top_k=3)\n</code></pre>"},{"location":"mle_hyperopt/#refining-the-search-space-of-your-strategy","title":"Refining the Search Space of Your Strategy \ud83e\ude93","text":"<pre><code># Refine the search space after 5 &amp; 10 iterations based on top 2 configurations\nstrategy = RandomSearch(real={\"lrate\": {\"begin\": 0.1,\n                                        \"end\": 0.5,\n                                        \"prior\": \"log-uniform\"}},\n                        integer={\"batch_size\": {\"begin\": 1,\n                                                \"end\": 5,\n                                                \"prior\": \"uniform\"}},\n                        categorical={\"arch\": [\"mlp\", \"cnn\"]},\n                        search_config={\"refine_after\": [5, 10],\n                                       \"refine_top_k\": 2})\n\n# Or do so manually using `refine` method\nstrategy.tell(...)\nstrategy.refine(top_k=2)\n</code></pre> <p>Note that the search space refinement is only implemented for random, SMBO and nevergrad-based search strategies.</p>"},{"location":"mle_logging/","title":"The <code>mle-logging</code> Package","text":""},{"location":"mle_logging/#experiment-logging-made-easy","title":"Experiment logging made easy \ud83d\udcd6","text":"<p>Each Python-based experiment is assumed to use a custom logger: The <code>MLELogger</code>. This enables the standardization needed to automatically aggregate multiple random runs and to log performance across hyperparameter searches. For a quickstart checkout the notebook blog \ud83d\ude80</p>"},{"location":"mle_logging/#the-api","title":"The API \ud83c\udfae","text":"<pre><code>from mle_logging import MLELogger\n\n# Instantiate logging to experiment_dir\nlog = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n                what_to_track=['train_loss', 'test_loss'],\n                experiment_dir=\"experiment_dir/\",\n                model_type='torch')\n\ntime_tic = {'num_updates': 10, 'num_epochs': 1}\nstats_tic = {'train_loss': 0.1234, 'test_loss': 0.1235}\n\n# Update the log with collected data &amp; save it to .hdf5\nlog.update(time_tic, stats_tic)\nlog.save()\n</code></pre> <p>You can also log model checkpoints, matplotlib figures and other <code>.pkl</code> compatible objects.</p> <pre><code># Save a model (torch, tensorflow, sklearn, jax, numpy)\nimport torchvision.models as models\nmodel = models.resnet18()\nlog.save_model(model)\n\n# Save a matplotlib figure as .png\nfig, ax = plt.subplots()\nlog.save_plot(fig)\n\n# You can also save (somewhat) arbitrary objects .pkl\nsome_dict = {\"hi\" : \"there\"}\nlog.save_extra(some_dict)\n</code></pre> <p>Or do everything in a single line... <pre><code>log.update(time_tic, stats_tic, model, fig, extra, save=True)\n</code></pre></p>"},{"location":"mle_logging/#file-structure-re-loading","title":"File Structure &amp; Re-Loading \ud83d\udcda","text":"<p>The <code>MLELogger</code> will create a nested directory, which looks as follows:</p> <pre><code>experiment_dir\n\u251c\u2500\u2500 extra: Stores saved .pkl object files\n\u251c\u2500\u2500 figures: Stores saved .png figures\n\u251c\u2500\u2500 logs: Stores .hdf5 log files (meta, stats, time)\n\u251c\u2500\u2500 models: Stores different model checkpoints\n    \u251c\u2500\u2500 init: Stores initial checkpoint\n    \u251c\u2500\u2500 final: Stores most recent checkpoint\n    \u251c\u2500\u2500 every_k: Stores every k-th checkpoint provided in update\n    \u251c\u2500\u2500 top_k: Stores portfolio of top-k checkpoints based on performance\n\u251c\u2500\u2500 tboards: Stores tensorboards for model checkpointing\n\u251c\u2500\u2500 &lt;config_name&gt;.json: Copy of configuration file (if provided)\n</code></pre> <p>For visualization and post-processing load the results via <pre><code>from mle_logging import load_log\nlog_out = load_log(\"experiment_dir/\")\n\n# The results can be accessed via meta, stats and time keys\n# &gt;&gt;&gt; log_out.meta.keys()\n# odict_keys(['experiment_dir', 'extra_storage_paths', 'fig_storage_paths', 'log_paths', 'model_ckpt', 'model_type'])\n# &gt;&gt;&gt; log_out.stats.keys()\n# odict_keys(['test_loss', 'train_loss'])\n# &gt;&gt;&gt; log_out.time.keys()\n# odict_keys(['time', 'num_epochs', 'num_updates', 'time_elapsed'])\n</code></pre></p> <p>If an experiment was aborted, you can reload and continue the previous run via the <code>reload=True</code> option:</p> <pre><code>log = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n                what_to_track=['train_loss', 'test_loss'],\n                experiment_dir=\"experiment_dir/\",\n                model_type='torch',\n                reload=True)\n</code></pre>"},{"location":"mle_logging/#installation","title":"Installation \u23f3","text":"<p>A PyPI installation is available via:</p> <pre><code>pip install mle-logging\n</code></pre> <p>Alternatively, you can clone this repository and afterwards 'manually' install it:</p> <pre><code>git clone https://github.com/RobertTLange/mle-logging.git\ncd mle-logging\npip install -e .\n</code></pre>"},{"location":"mle_logging/#advanced-options","title":"Advanced Options \ud83d\udeb4","text":""},{"location":"mle_logging/#merging-multiple-logs","title":"Merging Multiple Logs \ud83d\udc6b","text":"<p>Merging Multiple Random Seeds \ud83c\udf31 + \ud83c\udf31</p> <pre><code>from mle_logging import merge_seed_logs\nmerge_seed_logs(\"multi_seed.hdf\", \"experiment_dir/\")\nlog_out = load_log(\"experiment_dir/\")\n# &gt;&gt;&gt; log.eval_ids\n# ['seed_1', 'seed_2']\n</code></pre> <p>Merging Multiple Configurations \ud83d\udd16 + \ud83d\udd16</p> <pre><code>from mle_logging import merge_config_logs, load_meta_log\nmerge_config_logs(experiment_dir=\"experiment_dir/\",\n                  all_run_ids=[\"config_1\", \"config_2\"])\nmeta_log = load_meta_log(\"multi_config_dir/meta_log.hdf5\")\n# &gt;&gt;&gt; log.eval_ids\n# ['config_2', 'config_1']\n# &gt;&gt;&gt; meta_log.config_1.stats.test_loss.keys()\n# odict_keys(['mean', 'std', 'p50', 'p10', 'p25', 'p75', 'p90']))\n</code></pre>"},{"location":"mle_logging/#plotting-of-logs","title":"Plotting of Logs \ud83e\uddd1\u200d\ud83c\udfa8","text":"<pre><code>meta_log = load_meta_log(\"multi_config_dir/meta_log.hdf5\")\nmeta_log.plot(\"train_loss\", \"num_updates\")\n</code></pre>"},{"location":"mle_logging/#storing-checkpoint-portfolios","title":"Storing Checkpoint Portfolios \ud83d\udcc2","text":"<p>Logging every k-th checkpoint update \u2757 \u23e9 ... \u23e9 \u2757</p> <pre><code># Save every second checkpoint provided in log.update (stored in models/every_k)\nlog = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n                what_to_track=['train_loss', 'test_loss'],\n                experiment_dir='every_k_dir/',\n                model_type='torch',\n                ckpt_time_to_track='num_updates',\n                save_every_k_ckpt=2)\n</code></pre> <p>Logging top-k checkpoints based on metric \ud83d\udd31</p> <pre><code># Save top-3 checkpoints provided in log.update (stored in models/top_k)\n# Based on minimizing the test_loss metric\nlog = MLELogger(time_to_track=['num_updates', 'num_epochs'],\n                what_to_track=['train_loss', 'test_loss'],\n                experiment_dir=\"top_k_dir/\",\n                model_type='torch',\n                ckpt_time_to_track='num_updates',\n                save_top_k_ckpt=3,\n                top_k_metric_name=\"test_loss\",\n                top_k_minimize_metric=True)\n</code></pre>"},{"location":"mle_monitor/","title":"The <code>mle-monitor</code> Package","text":""},{"location":"mle_monitor/#resource-monitoring-made-easy","title":"Resource monitoring made easy \ud83d\udcfa","text":"<p>\"Did I already run this experiment before? How many resources are currently available on my cluster?\" If these are common questions you encounter during your daily life as a researcher, then <code>mle-monitor</code> is made for you. It provides a lightweight API for tracking your experiments using a pickle protocol database (e.g. for hyperparameter searches and/or multi-configuration/multi-seed runs). Furthermore, it comes with built-in resource monitoring on Slurm/Grid Engine clusters and local machines/servers.</p> <p><code>mle-monitor</code> provides three core functionalities:</p> <ul> <li><code>MLEProtocol</code>: A composable protocol database API for ML experiments.</li> <li><code>MLEResource</code>: A tool for obtaining server/cluster usage statistics.</li> <li><code>MLEDashboard</code>: A dashboard visualizing resource usage &amp; experiment protocol.</li> </ul> <p>To get started I recommend checking out the colab notebook and an example workflow.</p> <p></p>"},{"location":"mle_monitor/#mleprotocol-keeping-track-of-your-experiments","title":"<code>MLEProtocol</code>: Keeping Track of Your Experiments \ud83d\udcdd","text":"<pre><code>from mle_monitor import MLEProtocol\n\n# Load protocol database or create new one -&gt; print summary\nprotocol_db = MLEProtocol(\"mle_protocol.db\", verbose=False)\nprotocol_db.summary(tail=10, verbose=True)\n\n# Draft data to store in protocol &amp; add it to the protocol\nmeta_data = {\n    \"purpose\": \"Grid search\",  # Purpose of experiment\n    \"project_name\": \"MNIST\",  # Project name of experiment\n    \"experiment_type\": \"hyperparameter-search\",  # Type of experiment\n    \"experiment_dir\": \"experiments/logs\",  # Experiment directory\n    \"num_total_jobs\": 10,  # Number of total jobs to run\n    ...\n}\nnew_experiment_id = protocol_db.add(meta_data)\n\n# ... train your 10 (pseudo) networks/complete respective jobs\nfor i in range(10):\n    protocol_db.update_progress_bar(new_experiment_id)\n\n# Wrap up an experiment (store completion time, etc.)\nprotocol_db.complete(new_experiment_id)\n</code></pre> <p>The meta data can contain the following keys:</p> Search Type Description Default <code>purpose</code> Purpose of experiment <code>'None provided'</code> <code>project_name</code> Project name of experiment <code>'default'</code> <code>exec_resource</code> Resource jobs are run on <code>'local'</code> <code>experiment_dir</code> Experiment log storage directory <code>'experiments'</code> <code>experiment_type</code> Type of experiment to run <code>'single'</code> <code>base_fname</code> Main code script to execute <code>'main.py'</code> <code>config_fname</code> Config file path of experiment <code>'base_config.yaml'</code> <code>num_seeds</code> Number of evaluations seeds 1 <code>num_total_jobs</code> Number of total jobs to run 1 <code>num_job_batches</code> Number of jobs in single batch 1 <code>num_jobs_per_batch</code> Number of sequential job batches 1 <code>time_per_job</code> Expected duration: days-hours-minutes <code>'00:01:00'</code> <code>num_cpus</code> Number of CPUs used in job 1 <code>num_gpus</code> Number of GPUs used in job 0 <p>Additionally you can synchronize the protocol with a Google Cloud Storage (GCS) bucket by providing <code>cloud_settings</code>. In this case also the results stored in <code>experiment_dir</code> will be uploaded to the GCS bucket, when you call <code>protocol.complete()</code>.</p> <pre><code># Define GCS settings - requires 'GOOGLE_APPLICATION_CREDENTIALS' env var.\ncloud_settings = {\n    \"project_name\": \"mle-toolbox\",  # GCP project name\n    \"bucket_name\": \"mle-protocol\",  # GCS bucket name\n    \"use_protocol_sync\": True,  # Whether to sync the protocol to GCS\n    \"use_results_storage\": True,  # Whether to sync experiment_dir to GCS\n}\nprotocol_db = MLEProtocol(\"mle_protocol.db\", cloud_settings, verbose=True)\n</code></pre>"},{"location":"mle_monitor/#the-mleresource-keeping-track-of-your-resources","title":"The <code>MLEResource</code>: Keeping Track of Your Resources \ud83d\udcc9","text":""},{"location":"mle_monitor/#on-your-local-machine","title":"On Your Local Machine","text":"<pre><code>from mle_monitor import MLEResource\n\n# Instantiate local resource and get usage data\nresource = MLEResource(resource_name=\"local\")\nresource_data = resource.monitor()\n</code></pre>"},{"location":"mle_monitor/#on-a-slurm-cluster","title":"On a Slurm Cluster","text":"<pre><code>resource = MLEResource(\n    resource_name=\"slurm-cluster\",\n    monitor_config={\"partitions\": [\"&lt;partition-1&gt;\", \"&lt;partition-2&gt;\"]},\n)\n</code></pre>"},{"location":"mle_monitor/#on-a-grid-engine-cluster","title":"On a Grid Engine Cluster","text":"<pre><code>resource = MLEResource(\n    resource_name=\"sge-cluster\",\n    monitor_config={\"queues\": [\"&lt;queue-1&gt;\", \"&lt;queue-2&gt;\"]}\n)\n</code></pre>"},{"location":"mle_monitor/#the-mledashboard-dashboard-visualization","title":"The <code>MLEDashboard</code>: Dashboard Visualization \ud83c\udf9e\ufe0f","text":"<pre><code>from mle_monitor import MLEDashboard\n\n# Instantiate dashboard with protocol and resource\ndashboard = MLEDashboard(protocol, resource)\n\n# Get a static snapshot of the protocol &amp; resource utilisation printed in console\ndashboard.snapshot()\n\n# Run monitoring in while loop - dashboard\ndashboard.live()\n</code></pre>"},{"location":"mle_monitor/#installation","title":"Installation \u23f3","text":"<p>A PyPI installation is available via:</p> <pre><code>pip install mle-monitor\n</code></pre> <p>Alternatively, you can clone this repository and afterwards 'manually' install it:</p> <pre><code>git clone https://github.com/mle-infrastructure/mle-monitor.git\ncd mle-monitor\npip install -e .\n</code></pre>"},{"location":"mle_scheduler/","title":"The <code>mle-scheduler</code> Package","text":""},{"location":"mle_scheduler/#resource-allocation-made-easy","title":"Resource allocation made easy \ud83d\ude82","text":"<p><code>mle-scheduler</code> provides a lightweight API to launch and monitor job queues on Slurm/Open Grid Engine clusters, SSH servers or Google Cloud Platform VMs. It smoothly orchestrates simultaneous runs for different configurations and/or random seeds. It is meant to reduce boilerplate and to make job resource specification intuitive. It comes with two core pillars:</p> <ul> <li><code>MLEJob</code>: Launches and monitors a single job on a resource (Slurm, Open Grid Engine, GCP, SSH, etc.).</li> <li><code>MLEQueue</code>: Launches and monitors a queue of jobs with different training configurations and/or seeds.</li> </ul> <p>For a quickstart check out the notebook blog or the example scripts \ud83d\udcd6</p> Local Slurm Grid Engine SSH GCP <p></p>"},{"location":"mle_scheduler/#installation","title":"Installation \u23f3","text":"<pre><code>pip install mle-scheduler\n</code></pre>"},{"location":"mle_scheduler/#managing-a-single-job-with-mlejob-locally","title":"Managing a Single Job with <code>MLEJob</code> Locally \ud83d\ude80","text":"<pre><code>from mle_scheduler import MLEJob\n\n# python train.py -config base_config_1.yaml -exp_dir logs_single -seed_id 1\njob = MLEJob(\n    resource_to_run=\"local\",\n    job_filename=\"train.py\",\n    config_filename=\"base_config_1.yaml\",\n    experiment_dir=\"logs_single\",\n    seed_id=1\n)\n\n_ = job.run()\n</code></pre>"},{"location":"mle_scheduler/#managing-a-queue-of-jobs-with-mlequeue-locally","title":"Managing a Queue of Jobs with <code>MLEQueue</code> Locally \ud83d\ude80...\ud83d\ude80","text":"<pre><code>from mle_scheduler import MLEQueue\n\n# python train.py -config base_config_1.yaml -seed 0 -exp_dir logs_queue/&lt;date&gt;_base_config_1\n# python train.py -config base_config_1.yaml -seed 1 -exp_dir logs_queue/&lt;date&gt;_base_config_1\n# python train.py -config base_config_2.yaml -seed 0 -exp_dir logs_queue/&lt;date&gt;_base_config_2\n# python train.py -config base_config_2.yaml -seed 1 -exp_dir logs_queue/&lt;date&gt;_base_config_2\nqueue = MLEQueue(\n    resource_to_run=\"local\",\n    job_filename=\"train.py\",\n    config_filenames=[\"base_config_1.yaml\",\n                      \"base_config_2.yaml\"],\n    random_seeds=[0, 1],\n    experiment_dir=\"logs_queue\"\n)\n\nqueue.run()\n</code></pre>"},{"location":"mle_scheduler/#launching-slurm-cluster-based-jobs","title":"Launching Slurm Cluster-Based Jobs \ud83d\udc12","text":"<pre><code># Each job requests 5 CPU cores &amp; 1 V100S GPU &amp; loads CUDA 10.0\njob_args = {\n    \"partition\": \"&lt;SLURM_PARTITION&gt;\",  # Partition to schedule jobs on\n    \"env_name\": \"mle-toolbox\",  # Env to activate at job start-up\n    \"use_conda_venv\": True,  # Whether to use anaconda venv\n    \"num_logical_cores\": 5,  # Number of requested CPU cores per job\n    \"num_gpus\": 1,  # Number of requested GPUs per job\n    \"gpu_type\": \"V100S\",  # GPU model requested for each job\n    \"modules_to_load\": \"nvidia/cuda/10.0\"  # Modules to load at start-up\n}\n\nqueue = MLEQueue(\n    resource_to_run=\"slurm-cluster\",\n    job_filename=\"train.py\",\n    job_arguments=job_args,\n    config_filenames=[\"base_config_1.yaml\",\n                      \"base_config_2.yaml\"],\n    experiment_dir=\"logs_slurm\",\n    random_seeds=[0, 1]\n)\nqueue.run()\n</code></pre>"},{"location":"mle_scheduler/#launching-gridengine-cluster-based-jobs","title":"Launching GridEngine Cluster-Based Jobs \ud83d\udc18","text":"<pre><code># Each job requests 5 CPU cores &amp; 1 V100S GPU w. CUDA 10.0 loaded\njob_args = {\n    \"queue\": \"&lt;GRID_ENGINE_QUEUE&gt;\",  # Queue to schedule jobs on\n    \"env_name\": \"mle-toolbox\",  # Env to activate at job start-up\n    \"use_conda_venv\": True,  # Whether to use anaconda venv\n    \"num_logical_cores\": 5,  # Number of requested CPU cores per job\n    \"num_gpus\": 1,  # Number of requested GPUs per job\n    \"gpu_type\": \"V100S\",  # GPU model requested for each job\n    \"gpu_prefix\": \"cuda\"  #$ -l {gpu_prefix}=\"{num_gpus}\"\n}\n\nqueue = MLEQueue(\n    resource_to_run=\"slurm-cluster\",\n    job_filename=\"train.py\",\n    job_arguments=job_args,\n    config_filenames=[\"base_config_1.yaml\",\n                      \"base_config_2.yaml\"],\n    experiment_dir=\"logs_grid_engine\",\n    random_seeds=[0, 1]\n)\nqueue.run()\n</code></pre>"},{"location":"mle_scheduler/#launching-ssh-server-based-jobs","title":"Launching SSH Server-Based Jobs \ud83e\udd8a","text":"<pre><code>ssh_settings = {\n    \"user_name\": \"&lt;SSH_USER_NAME&gt;\",  # SSH server user name\n    \"pkey_path\": \"&lt;PKEY_PATH&gt;\",  # Private key path (e.g. ~/.ssh/id_rsa)\n    \"main_server\": \"&lt;SSH_SERVER&gt;\",  # SSH Server address\n    \"jump_server\": '',  # Jump host address\n    \"ssh_port\": 22,  # SSH port\n    \"remote_dir\": \"mle-code-dir\",  # Dir to sync code to on server\n    \"start_up_copy_dir\": True,  # Whether to copy code to server\n    \"clean_up_remote_dir\": True  # Whether to delete remote_dir on exit\n}\n\njob_args = {\n    \"env_name\": \"mle-toolbox\",  # Env to activate at job start-up\n    \"use_conda_venv\": True  # Whether to use anaconda venv\n}\n\nqueue = MLEQueue(\n    resource_to_run=\"ssh-node\",\n    job_filename=\"train.py\",\n    config_filenames=[\"base_config_1.yaml\",\n                      \"base_config_2.yaml\"],\n    random_seeds=[0, 1],\n    experiment_dir=\"logs_ssh_queue\",\n    job_arguments=job_args,\n    ssh_settings=ssh_settings)\n\nqueue.run()\n</code></pre>"},{"location":"mle_scheduler/#launching-gcp-vm-based-jobs","title":"Launching GCP VM-Based Jobs \ud83e\udd84","text":"<pre><code>cloud_settings = {\n    \"project_name\": \"&lt;GCP_PROJECT_NAME&gt;\",  # Name of your GCP project\n    \"bucket_name\": \"&lt;GCS_BUCKET_NAME&gt;\", # Name of your GCS bucket\n    \"remote_dir\": \"&lt;GCS_CODE_DIR_NAME&gt;\",  # Name of code dir in bucket\n    \"start_up_copy_dir\": True,  # Whether to copy code to bucket\n    \"clean_up_remote_dir\": True  # Whether to delete remote_dir on exit\n}\n\njob_args = {\n    \"num_gpus\": 0,  # Number of requested GPUs per job\n    \"gpu_type\": None,  # GPU requested e.g. \"nvidia-tesla-v100\"\n    \"num_logical_cores\": 1,  # Number of requested CPU cores per job\n}\n\nqueue = MLEQueue(\n    resource_to_run=\"gcp-cloud\",\n    job_filename=\"train.py\",\n    config_filenames=[\"base_config_1.yaml\",\n                      \"base_config_2.yaml\"],\n    random_seeds=[0, 1],\n    experiment_dir=\"logs_gcp_queue\",\n    job_arguments=job_args,\n    cloud_settings=cloud_settings,\n)\nqueue.run()\n</code></pre>"},{"location":"dev/future_plans/","title":"The Future of the Toolbox","text":"<p>You can find a couple things that need to be tackled in the issues of this project. Below is a quick overview of large milestones that could need your help:</p> <ul> <li> Make <code>mle init</code> beautiful/a smoother/more minimal experience.</li> <li> Better documentation via sphinx, code style and PEP setup.</li> <li> Automated env/container generation + clean up (delete if existing)</li> <li> Asynchronous job scheduling based on \"trigger event\".</li> <li> Core functionalities for Population-based training.<ul> <li> Exploit Strategies</li> <li> Explore Strategies</li> <li> <code>PBT_Manager</code></li> </ul> </li> <li> Multi-objective SMBO (pareto front improvements).<ul> <li> Based BOTorch with different acquisition functions.</li> <li> Make <code>BaseHyperOptimisation</code> more general/adaptive.</li> <li> Get rid of <code>scikit-optimize</code> unstable dependency.</li> </ul> </li> <li> Modular adding of remote cloud VM backends:<ul> <li> Google Cloud Platform VM instances</li> <li> Amazon Web Services</li> <li> Microsoft Azure</li> </ul> </li> <li> <code>mle-labortaory</code> Web UI/Server.<ul> <li> Based on streamlit</li> <li> Monitoring from everywhere with password protection</li> <li> Easy retrieval of results via click</li> <li> Easy report generation via click</li> <li> Launch experiments from UI interface</li> </ul> </li> <li> More tests in test suite for core features of toolbox.<ul> <li> Update existing integration tests</li> <li> Test the MLE_Logger</li> <li> Test log merging</li> </ul> </li> </ul>"},{"location":"dev/infrastructure/","title":"Toolbox Infrastructure","text":"<p>In this document you can learn everything about how to run experiments with the <code>mle-toolbox</code>. The <code>mle-toolbox</code> allows you to run different types of experiments locally or on an SGE cluster. You have to provide three inputs:</p> <ol> <li>An experiment/meta configuration <code>.yaml</code> file.</li> <li>A job configuration <code>.json</code>file.</li> <li>A python <code>.py</code> script that runs your training loop.</li> </ol> <p></p> <p>The only things you have to do is specify your desired experiment. The toolbox automatically detects whether you start an experiment with access to multiple compute nodes.</p> <ul> <li><code>train.py</code> takes three arguments: <code>-config</code>, <code>-seed</code>, <code>-exp_dir</code></li> <li>This includes the standard inputs to the training function (<code>model_config</code>, <code>train_config</code>, <code>log_config</code>) but can be otherwise generalised to your applications.</li> </ul> <p></p>"},{"location":"dev/infrastructure/#jobs-evals-and-experiments","title":"Jobs, Evals and Experiments","text":"<p>Throughout the toolbox we refer to different granularities of compute loads. It helps being familiar with what these refer to (from lowest to highest level of specification):</p> <ul> <li>job: A single submission process on resource (e.g. one seed for one configuration)</li> <li>eval: A single parameter configuration which can be executed/trained for multiple seeds (individual jobs!)</li> <li>experiment: An entire sequence of jobs to be executed (e.g. grid search with pre/post-processing)</li> </ul>"},{"location":"dev/infrastructure/#protocol-db-logging","title":"Protocol DB Logging","text":"<p>TBC</p>"},{"location":"dev/infrastructure/#rich-dashboard-monitoring","title":"Rich Dashboard Monitoring","text":"<p>TBC</p>"},{"location":"dev/infrastructure/#experiment-report-summarization","title":"Experiment Report Summarization","text":"<p>TBC</p>"},{"location":"dev/notes/","title":"Notes for Development","text":""},{"location":"dev/notes/#toolbox-philosophy-notes","title":"Toolbox Philosophy Notes","text":"<ul> <li>Biology protocol: Simply a recipe, or written design, for performing the experiment.<ol> <li>Purpose: Formal statement which encompasses your tested hypothesis.</li> <li>Materials: What are major items needed to carry out your experiment?     -&gt; Git commit hash of code repository</li> <li>Methods: How will you set up your experiment?     -&gt; Hash of base_config.json     -&gt; Hash of meta_config.yaml</li> <li>Controls: What are you going to compare your results with?     -&gt; Other experiment id or None if no direct comparison</li> <li>Data Interpretation: What will be done with the data once it is collected?     -&gt; Generate figures from the experiment results</li> </ol> </li> <li>Causality tools from Econometrics: Let's be scientific about assessing the impact of algorithmic modifications and performance comparisons.<ul> <li>Multiple testing corrections</li> <li>Difference-in-difference estimation - experiment class</li> <li>Power assessment and p-value computation with automatic seed recommendation</li> </ul> </li> </ul>"},{"location":"dev/notes/#notes-for-documentation","title":"Notes for documentation","text":"<ul> <li>Apply for 1000$ GCP credits: https://edu.google.com/programs/credits/research/?modal_active=none</li> </ul>"},{"location":"dev/notes/#sun-grid-engine","title":"Sun Grid Engine","text":"<ul> <li>Details on how to submit jobs with qsub</li> <li>More notes on the SGE system</li> </ul>"},{"location":"dev/notes/#slurm","title":"Slurm","text":"<ul> <li>Note that slurm cluster head node allows you to only run 128 processes parallel<ul> <li>Need to salloc into a node - figure out max running time!</li> <li><code>salloc --job-name \"InteractiveJob\" --cpus-per-task 8 --mem-per-cpu 1500 --time 20:00:00 --partition standard</code></li> <li>git seems to be not working. Remote connection?! add export var to .bashrc</li> </ul> </li> <li>On Slurm it can make sense to start up a job for the experiment management in a screen/tmux session for monitoring of many jobs: <pre><code>screen\nsrun --job-name \"InteractiveJob\" --cpus-per-task 1 --mem-per-cpu 1500 --time 01:00:00 --partition standard --pty bash\n</code></pre></li> </ul>"},{"location":"dev/notes/#google-cloud-storage","title":"Google Cloud Storage","text":"<ul> <li> <p>Checkout Google Storage Python API: https://googleapis.dev/python/storage/latest/blobs.html</p> <ul> <li>How to use gcloud with a proxy server https://stackoverflow.com/questions/43926668/python3-bigquery-or-google-cloud-python-through-http-proxy/43945207#43945207</li> </ul> </li> <li> <p>How to set up Google Cloud Storage of the experiment log files</p> <ul> <li>Create a new project</li> <li>Create a new bucket in the project</li> <li>Set up an authentication key: https://cloud.google.com/docs/authentication/production#passing_variable</li> <li><code>pip install google-cloud-storage</code> and export the Google authentification bath in your .bashrc script</li> <li>Add credentials path to cluster_config.py file &amp; add the project + bucket name</li> <li>Set option whether entire experiment/figure directory should be stored!</li> </ul> </li> </ul>"},{"location":"dev/notes/#toolbox-dependencies","title":"Toolbox Dependencies","text":"<ul> <li>Pickle DB docs: https://patx.github.io/pickledb/commands.html</li> </ul>"},{"location":"dev/notes/#where-to-run-examples-tests-from","title":"Where to run examples &amp; tests from","text":"<ul> <li>Examples from the <code>mle_toolbox/examples/</code> directory.</li> <li>Tests from the <code>mle_toolbox/</code> directory</li> </ul>"},{"location":"dev/notes/#update-docs-homepage","title":"Update docs homepage","text":"<ul> <li>https://squidfunk.github.io/mkdocs-material/creating-your-site/</li> <li>https://github.com/squidfunk/mkdocs-material</li> </ul> <pre><code>pip install mkdocs-material\nmkdocs serve\nmkdocs gh-deploy --force\n</code></pre>"},{"location":"dev/notes/#github-actions","title":"GitHub Actions","text":"<ul> <li>Billing: https://docs.github.com/en/billing/managing-billing-for-github-actions/about-billing-for-github-actions</li> </ul>"},{"location":"dev/notes/#nice-visualization-tools","title":"Nice visualization tools","text":"<ul> <li>https://favicon.io/favicon-generator/ - For homepage MLE icon</li> <li>https://carbon.now.sh/ - for code screenshots</li> <li>https://github.com/homeport/termshot - for terminal output screenshots</li> </ul>"},{"location":"dev/remote_backends/","title":"Adding Remote Backends","text":""},{"location":"dev/testing/","title":"Running The Test Suite","text":""},{"location":"dev/testing/#flake8-linting","title":"<code>flake8</code> Linting","text":"<pre><code>flake8 ./mle_toolbox --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 ./mle_toolbox --count --exit-zero --max-line-length=127 --statistics\n</code></pre>"},{"location":"dev/testing/#mypy-type-checking","title":"<code>mypy</code> Type Checking","text":"<pre><code>mypy mle_toolbox/.\n</code></pre>"},{"location":"dev/testing/#black-formatting","title":"<code>black</code> Formatting","text":"<pre><code>black mle_toolbox/. --verbose\n</code></pre>"},{"location":"dev/testing/#test-coverage","title":"Test Coverage","text":"<p>Note: This page and content is still work in progress!</p>"},{"location":"dev/testing/#unit-tests","title":"Unit Tests","text":"<pre><code>pytest -vv tests/unit\n</code></pre> <ul> <li> <p> File loading: <code>tests/unit/test_load_files.py</code></p> <ul> <li> <code>.yaml</code> experiment configuration</li> <li> <code>.json</code> run configuration</li> <li> <code>meta_log.hdf5</code></li> <li> <code>hyper_log.pkl</code></li> <li> Trained models<ul> <li> <code>.pkl</code>-based (sklearn)</li> <li> <code>.npy</code>-based (JAX)</li> <li> <code>.pt</code>-based (PyTorch)</li> </ul> </li> </ul> </li> <li> <p> Experiment launch configuration file generation</p> <ul> <li> <code>.qsub</code></li> <li> <code>.sbash</code></li> <li> GCP-startup <code>.sh</code> file</li> </ul> </li> <li> <p> Logging</p> <ul> <li> Individual run<ul> <li> assert key errors</li> <li> directory creation - correct file structure</li> <li> are files stored in correct location?</li> <li> is data correctly stored?</li> </ul> </li> <li> Merging into <code>meta_log.hdf5</code></li> <li> Summary into <code>hyper_log.pkl</code></li> <li> Reloading correctly</li> <li> All data types supported</li> <li> Tensorboard support</li> <li> Image storage support</li> </ul> </li> <li> <p> Experiment Protocol Logging</p> <ul> <li> Adding a new experiment</li> <li> Deleting a failed experiment</li> </ul> </li> </ul>"},{"location":"dev/testing/#integration-tests","title":"Integration Tests","text":"<pre><code>pytest -vv tests/integration\n</code></pre> <ul> <li> <p> Experiment types running on different resources</p> <ul> <li> Single configuration: <code>tests/integration/test_single_config.py</code></li> <li> Multiple configuration: <code>tests/integration/test_multi_configs.py</code></li> <li> A/Synchronous Grid search experiment: <code>tests/integration/test_grid_search.py</code></li> <li> Random search experiment: <code>tests/integration/test_random_search.py</code></li> <li> SMBO search experiment: <code>tests/integration/test_smbo_search.py</code></li> <li> PBT experiment</li> </ul> </li> <li> <p> Report generation</p> <ul> <li> Figure generation from meta-log</li> <li> Figure generation from hyper-log</li> <li> .md generation</li> </ul> </li> <li> <p> Results retrieval</p> <ul> <li> From GCS bucket</li> <li> From remote resource</li> </ul> </li> <li> <p> Toolbox initialization</p> <ul> <li> Config file changing</li> <li> Encryption ssh credentials</li> </ul> </li> <li> <p> GCS integration</p> <ul> <li> Pull dummy protocol DB</li> <li> Send results/retrieve results</li> </ul> </li> </ul>"},{"location":"mle_toolbox/configuration/","title":"Toolbox Configuration","text":"<p>By default the toolbox will run locally and without any GCS bucket backup of your experiment results. Furthermore, a lightweight PickleDB protocol database of your experiments will not be synced with the cloud version. In the following, we walkthrough how to</p> <ol> <li>Enable the execution of jobs on remote resources (cluster/storage) from your local machine or from the resource itself.</li> <li>Enable the backing up of your experiment results in a GCS bucket.</li> <li>Enable the backing up of a PickleDB experiment meta log in a GCS bucket.</li> <li>Enable resource monitoring and online dashboard visualization.</li> <li>Enable slack bot notifications for experiment completion and reporting.</li> </ol> <p>Note: There are two ways to perform the toolbox configuration:</p> <ol> <li>After installation execute <code>mle init</code>. This will walk you through all configuration steps in your CLI and save your configuration in <code>~/mle_config.toml</code>.</li> <li>Manually edit the <code>config_template.toml</code> template. Move/rename the template to your home directory via <code>mv config_template.toml ~/mle_config.toml</code>.</li> </ol> <p>The configuration procedure consists of 4 optional steps, which depend on your needs:</p> <ol> <li>Set whether to store all results &amp; your database locally or remote in a GCS bucket.</li> <li>Add SGE and/or Slurm credentials &amp; cluster-specific details (headnode, partitions, proxy server, etc.).</li> <li>Add the GCP project, GCS bucket name and database filename to store your results.</li> <li>Add credentials for a slack bot integration that notifies you about the state of your experiments.</li> </ol>"},{"location":"mle_toolbox/configuration/#general-settings","title":"General Settings \ud83e\uddbf","text":"<p>The configuration starts with a set of basic settings such as the path to your local experiment protocol pickle database, the path to your private key (for passwordless scp/rsync/etc.), environment settings and whether you would like to use slack bot notifications:</p> <pre><code>#------------------------------------------------------------------------------#\n# 1. General Toolbox Configuration - Verbosity + Whether to use GCS Storage\n#------------------------------------------------------------------------------#\n[general]\n# Local filename to store protocol DB in\nlocal_protocol_fname = '~/local_mle_protocol.db'\n\n# Path to private key for passwordless SSH access\npkey_path = '~/.ssh/id_rsa'\n\n# Whether to use conda or venv for experiment virtual environment\nuse_conda_venv = true\nuse_venv_venv = false\n\n# Whether to use slack bot notifications\nuse_slack_bot = false\n\n# Set remote experiment submission environment name\nremote_env_name = 'mle-toolbox'\n\n# Set the random seed for the toolbox/full experiment reproducibility\nrandom_seed = 42\n</code></pre> <p>You can also set the random seed for reproducibility of your hyperparameter search schedules.</p>"},{"location":"mle_toolbox/configuration/#remote-resource-execution","title":"Remote Resource Execution \ud83c\udfc3","text":"<p>The toolbox supports the usage of multiple different compute resources. This includes your local machine, but more importantly remote clusters such as the prominent Slurm and Grid Engine schedulers and Google Cloud Platform VMs. In order to be able to schedule remote jobs from your local machine or retrieve the results from the cluster, you will have to provide your credentials, headnode and partition names as well as some default arguments for jobs:</p> <pre><code>#------------------------------------------------------------------------------#\n# 2. Configuration for Slurm Cluster\n#------------------------------------------------------------------------------#\n[slurm]\n# Slurm credentials - Only if you want to retrieve results from cluster\n[slurm.credentials]\nuser_name = '&lt;slurm-user-name&gt;'\n\n# Slurm cluster information - Job scheduling, monitoring &amp; retrieval\n[slurm.info]\n# Partitions to monitor/run jobs on\npartitions = ['&lt;partition1&gt;']\n\n# Info for results retrieval &amp; internet tunneling if local launch\nmain_server_name = '&lt;main-server-ip&gt;'   # E.g. 'gateway.hpc.tu-berlin.de'\njump_server_name = '&lt;jump-host-ip&gt;'   # E.g. 'sshgate.tu-berlin.de'\nssh_port = 22\n# Add proxy server for internet connection if needed!\nhttp_proxy = \"http://&lt;slurm_headnode&gt;:3128/\"  # E.g. 'http://frontend01:3128/'\nhttps_proxy = \"http://&lt;slurm_headnode&gt;:3128/\"  # E.g. 'http://frontend01:3128/'\n\n# Default Slurm job arguments (if not supplied in job .yaml config)\n[slurm.default_job_args]\nnum_logical_cores = 2\ngpu_tpye = \"tesla\"\npartition = '&lt;partition1&gt;'\njob_name = 'temp'\nlog_file = 'log'\nerr_file = 'err'\nenv_name = '&lt;mle-default-env&gt;'\n</code></pre>"},{"location":"mle_toolbox/configuration/#google-cloud-platform","title":"Google Cloud Platform \u2601\ufe0f","text":"<p>If you want to use the toolbox for orchestrating GCP VMs, you will need to have set up the Google Cloud SDK. This will work as follows:</p> <pre><code>curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-353.0.0-linux-x86_64.tar.gz\n./google-cloud-sdk/install.sh\n./google-cloud-sdk/bin/gcloud init\n</code></pre> <p>At initialization you will be required to select a GCP project. Internally the toolbox will call different <code>gcloud</code> commands to launch new VMs and/or monitor the status of running jobs.</p>"},{"location":"mle_toolbox/configuration/#gcs-bucket-backups","title":"GCS Bucket Backups \ud83d\uddf3\ufe0f","text":"<p>If you choose so, the toolbox will sync a local version of your experiment protocol database with a GCS bucket. Furthermore, the results of your experiments will be zipped and stored via a unique hash. You can afterwards use <code>mle retrieve</code> in order to retrieve these backed up results from the bucket. These functionalities rely on <code>google-cloud-storage</code> and you having set your <code>GOOGLE_APPLICATION_CREDENTIALS</code>. You have to follow four steps:</p> <ol> <li>Create your <code>.json</code> key file here. Name it <code>~/gcp_mle_key.json</code></li> <li>Set the path via adding <code>export GOOGLE_APPLICATION_CREDENTIALS = ~/gcp_mle_key.json</code> to your <code>.bashrc</code> or <code>.zshrc</code> file.</li> <li>Create a GCP project and a storage bucket. Have a look here for how to do this.</li> <li>Provide the path, GCP project and bucket to store your results in your <code>~/mle_config.toml</code> configuration:</li> </ol> <pre><code>#------------------------------------------------------------------------------#\n# 4. GCP Config - Credentials, Project + Buffer for Meta-Experiment Protocol\n# OPTIONAL: Only required if you want to sync protocol with GCS\n#------------------------------------------------------------------------------#\n[gcp]\n# Set GCloud project and bucket name for storage/compute instances\nproject_name = \"&lt;gcloud_project_name&gt;\"\nbucket_name = \"&lt;gcloud_bucket_name&gt;\"\n\n# Filename to retrieve from gcloud bucket &amp; where to store\nprotocol_fname = \"gcloud_mle_protocol.db\"\n\n# Syncing of protocol with GCloud\nuse_protocol_sync = false\n\n# Storing of experiment results in bucket\nuse_results_storage = false\n\n# Default GCP job arguments (if not differently supplied)\n[gcp.default_job_arguments]\nnum_logical_cores = 2\nnum_gpus = 0\nuse_tpus = false\njob_name = 'temp'\nlog_file = 'log'\nerr_file = 'err'\n</code></pre>"},{"location":"mle_toolbox/configuration/#slack-bot-notification","title":"Slack Bot Notification \ud83d\udd08","text":"<p>If you want to, you can add slack notifications using the <code>slack-clusterbot</code>. If you want to learn more about how to set it up, checkout the wiki documentation. To make it short, you need to create a Bot User OAuth Access Token for your slack workspace and afterwards add this to your <code>mle_config.toml</code>:</p> <pre><code>#------------------------------------------------------------------------------#\n# 5. Slack Bot Config - OAuth Access Token &amp; Config Path\n# https://github.com/sprekelerlab/slack-clusterbot/wiki/Installation\n# OPTIONAL: Only required if you want to slack notifications/updates\n#------------------------------------------------------------------------------#\n[slack]\n# Set authentication token and default username messages are sent to\nslack_token = \"&lt;xyz-token&gt;\"\nuser_name = \"&lt;user_name&gt;\"\n</code></pre>"},{"location":"mle_toolbox/experiments/","title":"Experiment Configuration","text":"<p>Each experiment you want to execute via <code>mle run</code> has to be specified via a <code>.yaml</code> file. This file has to contain a set of basic ingredients such as meta arguments (which script to execute, etc.) - <code>meta_job_args</code>, the experiment type-specific arguments and resource/single job-specific arguments - <code>single_job_args</code>.</p>"},{"location":"mle_toolbox/experiments/#basic-configuration","title":"Basic Configuration","text":"<p>The <code>meta_job_args</code> specify the project name, experiment type, directory and base job script/configuration. For a hyperparameter search experiment it should look as follows:</p> <pre><code># Meta Arguments: What job? What train .py file? Base config? Where to store?\nmeta_job_args:\nproject_name: \"&lt;project_name&gt;\"  # Project name for protocol db\nexperiment_type: \"&lt;experiment_type&gt;\"  # ['hyperparameter-search', 'multiple-configs', 'single-config', 'population-based-training']\nexperiment_dir: \"&lt;experiment_dir&gt;\"  # Directory to store results in\nbase_train_fname: \"&lt;train&gt;.py\"  # Base .py/.sh job execution script\nbase_train_config: \"&lt;train_config&gt;.yaml\"  # Base .json/.yaml job configuration\n</code></pre> <p>The <code>job_spec_args</code>, on the other hand, specifies environment settings and the resources required per individual job. E.g.:</p> <pre><code># Parameters specific to an individual job\nsingle_job_args:\njob_name: \"&lt;jname&gt;\"  # Job name prefix used for qsub/sbatch\nnum_logical_cores: 2  # Number of logical cpu cores\nnum_gpus: 1  # Number of gpus required per job\nenv_name: \"&lt;env_name&gt;\"  # Name of virtual environment to activat at startup\nuse_conda_venv: true  # Whether to use a conda environment\ntime_per_job: \"&lt;dd:hh:mm&gt;\"  # Time limits of a job\n</code></pre> <p>Given a fixed training configuration file and the Python training script, the only thing that has to be adapted for the different types of experiments are the experiment type-specific arguments. These include:</p> <ul> <li><code>multi_config_args</code>: For multi-configuration &amp; multi-seed experiments</li> <li><code>param_search_args</code>: For hyperparameter search experiments</li> <li><code>pbt_args</code>: For Population-Based Training</li> </ul>"},{"location":"mle_toolbox/experiments/#multiple-configurations-seeds","title":"Multiple Configurations &amp; Seeds","text":"<pre><code>multi_config_args:\nconfig_fnames:  # List of configuration files to execute\n- \"config_1.json\"\n- \"config_2.json\"\nnum_seeds: 2  # Number of seeds to schedule\n</code></pre>"},{"location":"mle_toolbox/experiments/#hyperparameter-search","title":"Hyperparameter Search","text":"<pre><code># Parameters specific to the hyperparameter search\nparam_search_args:\nsearch_logging:  # Logging-specific settings\nreload_log: False  # Load a previous hyper log\nverbose_log: True  # Print intermediate batch results\nmax_objective: False  # Whether to max or min objective\naggregate_seeds: \"p50\"  # Aggregation metric across seeds\nproblem_type: \"final\"  # How to score configs ('final', 'best', 'mean')\neval_metrics: \"test_loss\"  # Metric to look up for scoring\nsearch_resources:  # Resource scheduling settings\n# num_search_batches: 4\n# num_evals_per_batch: 4\nnum_total_evals: 4  # Number of total evaluations\nmax_running_jobs: 4  # Number of max jobs running at one time\nnum_seeds_per_eval: 1  # Number of seeds per evaluation\nrandom_seeds: [2]  # Explicitly fix seeds to run\nsearch_config:  # Search strategy-specific settings\nsearch_type: \"grid\"\n# search_schedule: \"sync\"\nsearch_schedule: \"async\"  # \"sync\" or \"async\" job/batch scheduling\nsearch_params:\nreal:\nlrate:\nbegin: 0.1\nend: 0.5\nbins: 2\ninteger:\nbatch_size:\nbegin: 1\nend: 5\nbins: 2\n</code></pre>"},{"location":"mle_toolbox/experiments/#population-based-training","title":"Population-Based Training","text":"<p>Note: Population-Based Training is still experimental and will most likely change. More updates to follow. Most likely the toolbox will only support a synchronous version.</p> <pre><code># Parameters specific to the population-based training\npbt_args:\npbt_logging:\nmax_objective: False\neval_metric: \"test_loss\"\npbt_resources:\nnum_population_members: 10\nnum_total_update_steps: 2000\nnum_steps_until_ready: 500\nnum_steps_until_eval: 100\npbt_config:\npbt_params:\nreal:\nl_rate:\nbegin: 1e-5\nend: 1e-2\nexploration:\nstrategy: \"perturb\"\nselection:\nstrategy: \"truncation\"\n</code></pre>"},{"location":"mle_toolbox/experiments/#pre-post-processing","title":"Pre-/Post-Processing","text":"<p>You can also add <code>pre_processing_args</code> and <code>post_processing_args</code> if you would like to either launch a job before or after the main experiment. This can be useful if you need to prepare some data that is afterwards used by all main experiment jobs or if you want to run some analysis using the networks trained in the main experiment.</p> <pre><code># Parameters for the pre-processing job\npre_processing_args:\nprocessing_fname: \"&lt;run_preprocessing&gt;.py\"\nprocessing_job_args:\nnum_logical_cores: 2\ntime_per_job: \"00:01:00\"\nextra_cmd_line_input:\nfigures_dir: \"experiments/data\"\n\n# Parameters for the post-processing job\npost_processing_args:\nprocessing_fname: \"&lt;run_postprocessing&gt;.py\"\nprocessing_job_args:\nnum_logical_cores: 2\ntime_per_job: \"00:01:00\"\nextra_cmd_line_input:\nfigures_dir: \"experiments/figures\"\n</code></pre>"},{"location":"mle_toolbox/getting_started/","title":"Video Tutorials","text":"<p>Note: I gave a general overview talk on the setup and usage of the toolbox:</p> <ul> <li>TODO: You can watch the talk here.</li> <li>You can have a look at the slide deck here.</li> </ul>"},{"location":"mle_toolbox/getting_started/#1-motivation-philosophy","title":"1. Motivation &amp; Philosophy","text":""},{"location":"mle_toolbox/getting_started/#2-subpackage-tutorial-walkthrough","title":"2. Subpackage Tutorial Walkthrough","text":""},{"location":"mle_toolbox/getting_started/#3-installation-credentials-setup","title":"3. Installation &amp; Credentials Setup","text":""},{"location":"mle_toolbox/getting_started/#4-different-mle-subcommand","title":"4. Different <code>mle &lt;subcommand&gt;</code>","text":""},{"location":"mle_toolbox/getting_started/#5-analyzing-experiment-results","title":"5. Analyzing Experiment Results","text":"<ul> <li> Manual inspection of <code>hyper_log</code> and <code>meta_log</code>.</li> </ul>"},{"location":"mle_toolbox/subcommands/","title":"MLE Subcommands","text":"<p>The main entry point for using the <code>mle-toolbox</code> is via its subcommands. Similar to how you would call <code>git &lt;subcmd&gt;</code>, you can use the different functionalities of the toolbox via <code>mle &lt;subcmd&gt;</code>. In the following we will go through all of the available subcommands and disuss their usage and optional inputs.</p>"},{"location":"mle_toolbox/subcommands/#mle-run-experiment-execution","title":"<code>mle run</code> - Experiment Execution \ud83d\ude80","text":"<p>An experiment can be launched from the command line via:</p> <pre><code>mle run &lt;experiment_config&gt;.yaml\n</code></pre> <p>The <code>&lt;experiment_config&gt;.yaml</code> file provides all experiment settings such as the experiment type and resources required per individual job. Read more about how to configure it here. You can add several command line options, which can come in handy when debugging or if you want to launch multiple experiments sequentially:</p> <ul> <li><code>--no_welcome</code>: Don't print welcome messages at experiment launch.</li> <li><code>--no_protocol</code>: Do not record experiment in the PickleDB protocol database.</li> <li><code>--resource_to_run &lt;resource&gt;</code>: Run the experiment on the specified resource.</li> <li><code>--purpose</code>: String purpose to store in the protocol database.</li> </ul> <p>The <code>--purpose</code> option is especially useful if you would like execute multiple experiments sequentially. In this case you would like to run a bash script and not have to specify the purpose at launch time. E.g.</p> <pre><code>mle run exp_1.yaml --purpose Experiment 1 Grid\nmle run exp_2.yaml --purpose Experiment 2 Grid\n</code></pre> <p>By default the jobs will run on your local machine. If you specify a different <code>&lt;resource&gt;</code> you will be asked whether you would like to sync your code with a remote directory. Afterwards, the experiment will continue to be executed on that resource.</p> <p></p>"},{"location":"mle_toolbox/subcommands/#mle-monitor-monitor-resources","title":"<code>mle monitor</code> - Monitor Resources \ud83d\udda5\ufe0f","text":"<p><code>mle monitor</code> provides a simple wrapper around the <code>mle-monitor</code> dashboard. It will load the protocol database using the configuration settings provided in <code>~/mle_config.toml</code> and the resource you are currently working on. Afterwards, the dashboard will be updated in real time and synchronized with most recent protocol from the GCS bucket every other minute.</p> <p></p>"},{"location":"mle_toolbox/subcommands/#mle-retrieve-retrieve-results","title":"<code>mle retrieve</code> - Retrieve Results \ud83d\udce5","text":"<p>Once your experiment has finished, you can retrieve its results (logging directory) using the <code>mle retrieve</code> subcommand. This will fetch the corresponding remote/GCS storage path from the protocol database and then copy the results over.</p> <p></p>"},{"location":"mle_toolbox/subcommands/#mle-report-report-results","title":"<code>mle report</code> - Report Results \ud83d\udc8c","text":"<p><code>mle report</code> will create a set of report files (<code>.html</code>, <code>.md</code> &amp; <code>.pdf</code>) corresponding to your experiment. The files will contain the experiment configuration and figures generated from the experiment logs. Alternatively, you can also set <code>report_generation</code> to <code>true</code> in the <code>meta_job_args</code> of your experiment <code>.yaml</code> file. In this case the report will be automatically created once your experiment is completed. If you are using the slack bot notification feature the report will be send to your slack account.</p>"},{"location":"mle_toolbox/subcommands/#mle-init-setup-toolbox-configuration","title":"<code>mle init</code> - Setup Toolbox Configuration \u23f3","text":"<p>When you first start to setup the toolbox, you can run <code>mle init</code> in order to create a <code>mle_config.toml</code> file that is stored in your root directory. It automatically launch your favorite editor so that you can modify your configuration settings.</p> <p></p>"},{"location":"mle_toolbox/subcommands/#mle-sync-download-gcs-backed-experiments","title":"<code>mle sync</code> - Download GCS-Backed Experiments \ud83d\udd04","text":"<p>If you have run multiple experiments over night and you would like to retrieve all experiments from your GCS bucket which were not synced previously, you can simply run <code>mle sync</code>. The toolbox will retrieve all zipped experiment directories and unpack them for you.</p>"},{"location":"mle_toolbox/subcommands/#mle-project-initialize-new-template-project","title":"<code>mle project</code> - Initialize New Template Project \ud83d\uddc2","text":"<p>I noticed that most of my projects follow the same file structure. It is depicted in the <code>mle-project</code> repository. If you run <code>mle project</code>, the toolbox will clone the repository, delete all git files and rename the directory, so that you can directly get started coding up your project with MLE-Infrastructure support.</p> <p></p>"},{"location":"mle_toolbox/subcommands/#mle-protocol-list-state-of-protocol","title":"<code>mle protocol</code> - List State of Protocol \ud83d\udcdd","text":"<p>You can get a snapshot of your most recent experiments which were protocolled by running <code>mle_protocol</code>. This can be useful if you don't want to run the dashboard, but simply want to get a quick overview.</p> <p></p>"},{"location":"mle_toolbox/toolbox/","title":"MLE-Toolbox Overview","text":"<p>ML researchers need to coordinate different types of experiments on separate remote resources. The Machine Learning Experiment (MLE)-Toolbox is designed to facilitate the workflow by providing a simple interface, standardized logging, many common ML experiment types (multi-seed/configurations, grid-searches and hyperparameter optimization pipelines). You can run experiments on your local machine, high-performance compute clusters (Slurm and Sun Grid Engine) as well as on cloud VMs (GCP). The results are archived (locally/GCS bucket) and can easily be retrieved or automatically summarized/reported.</p> <p></p>"},{"location":"mle_toolbox/toolbox/#what-does-the-mle-toolbox-provide","title":"What Does The <code>mle-toolbox</code> Provide? \ud83e\uddd1\u200d\ud83d\udd27","text":"<ol> <li>API for launching jobs on cluster/cloud computing platforms (Slurm, GridEngine, GCP).</li> <li>Common machine learning research experiment setups:<ul> <li>Launching and collecting multiple random seeds in parallel/batches or async.</li> <li>Hyperparameter searches: Random, Grid, SMBO, PBT and Nevergrad.</li> <li>Pre- and post-processing pipelines for data preparation/result visualization.</li> </ul> </li> <li>Automated report generation for hyperparameter search experiments.</li> <li>Storage/retrieval of results and database in Google Cloud Storage Bucket.</li> <li>Resource monitoring with dashboard visualization.</li> </ol>"},{"location":"mle_toolbox/toolbox/#the-4-step-mle-toolbox-cooking-recipe","title":"The 4 Step <code>mle-toolbox</code> Cooking Recipe \ud83c\udf72","text":"<ol> <li>Follow the instructions below to install the <code>mle-toolbox</code> and set up your credentials/configuration.</li> <li>Read the docs explaining the pillars of the toolbox &amp; the experiment meta-configuration job <code>.yaml</code> files .</li> <li>Learn more about the individual infrastructure subpackages with the dedicated tutorial.</li> <li>Check out the examples \ud83d\udcc4 to get started: Single Objective Optimization, Multi Objective Optimization.</li> <li>Run your own experiments using the template files, project and <code>mle run</code>.</li> </ol>"},{"location":"mle_toolbox/toolbox/#installation","title":"Installation \u23f3","text":"<p>If you want to use the toolbox on your local machine follow the instructions locally. Otherwise do so on your respective cluster resource (Slurm/SGE). A PyPI installation is available via:</p> <pre><code>pip install mle-toolbox\n</code></pre> <p>Alternatively, you can clone this repository and afterwards 'manually' install it:</p> <pre><code>git clone https://github.com/mle-infrastructure/mle-toolbox.git\ncd mle-toolbox\npip install -e .\n</code></pre>"},{"location":"mle_toolbox/toolbox/#setting-up-your-toolbox-configuration","title":"Setting Up Your Toolbox Configuration \ud83e\uddd1\u200d\ud83c\udfa8","text":"<p>By default the toolbox will support local runs without any GCS storage of your experiments. If you want to integrate the <code>mle-toolbox</code> with your SGE/Slurm clusters, you have to provide additional data. There 2 ways to do so:</p> <ol> <li>After installation type <code>mle init</code>. This will walk you through all configuration steps in your CLI and save your configuration in <code>~/mle_config.toml</code>.</li> <li>Manually edit the <code>config_template.toml</code> template. Move/rename the template to your home directory via <code>mv config_template.toml ~/mle_config.toml</code>.</li> </ol> <p>The configuration procedure consists of 4 optional steps, which depend on your needs:</p> <ol> <li>Set whether to store all results &amp; your database locally or remote in a GCS bucket.</li> <li>Add SGE and/or Slurm credentials &amp; cluster-specific details (headnode, partitions, proxy server, etc.).</li> <li>Add the GCP project, GCS bucket name and database filename to store your results.</li> <li>Add credentials for a slack bot integration that notifies you about the state of your experiments.</li> </ol>"},{"location":"mle_toolbox/toolbox/#the-core-toolbox-subcommands","title":"The Core Toolbox Subcommands \ud83c\udf31","text":"<p>You are now ready to dive deeper into the specifics of experiment configuration and can start running your first experiments from the cluster (or locally on your machine) with the following commands:</p> Command Description \ud83d\ude80 <code>mle run</code> Start up an experiment (multi-config/seeds, search). \ud83d\udda5\ufe0f <code>mle monitor</code> Monitor resource utilisation (<code>mle-monitor</code> wrapper). \ud83d\udce5 <code>mle retrieve</code> Retrieve experiment result from GCS/cluster. \ud83d\udc8c <code>mle report</code> Create an experiment report with figures. \u23f3 <code>mle init</code> Setup of credentials &amp; toolbox settings. \ud83d\udd04 <code>mle sync</code> Extract all GCS-stored results to your local drive. \ud83d\uddc2 <code>mle project</code> Initialize a new project by cloning <code>mle-project</code>. \ud83d\udcdd <code>mle protocol</code> List a summary of the most recent experiments. <p>You can find more documentation for each subcommand here.</p>"}]}