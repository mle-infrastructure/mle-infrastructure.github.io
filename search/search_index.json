{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the MLE-Infrastructure \ud83d\udd2c Experiment Logging Parameter Searches Experiment Launch Experiment Protocol Experiment Manager mle-logging mle-hyperopt mle-scheduler mle-monitor mle-toolbox The MLE-Infrastructure provides a reproducible workflow for distributed Machine Learning experimentation (MLE) with minimal engineering overhead. The core consists of 5 packages: mle-logging : Experiment logging with easy multi-seed and configuration aggregation. mle-hyperopt : Hyperparameter Optimization with config export, refinement & reloading. mle-monitor : Monitor cluster/cloud VM resource utilization & protocol experiments. mle-scheduler : Schedule & monitor jobs on Slurm, GridEngine clusters & GCP VMs. mle-toolbox : Glues everything together to manage & post-process experiments. Note I : A template repository of an infrastructure-based project can be found in the mle-project . You can inspect your experiment stack in an interactive web UI: mle-laboratory . Note II : mle-logging , mle-hyperopt , mle-monitor and mle-scheduler are standalone packages and can be used independently of the utilities provided by the mle-toolbox .","title":"Home"},{"location":"#welcome-to-the-mle-infrastructure","text":"Experiment Logging Parameter Searches Experiment Launch Experiment Protocol Experiment Manager mle-logging mle-hyperopt mle-scheduler mle-monitor mle-toolbox The MLE-Infrastructure provides a reproducible workflow for distributed Machine Learning experimentation (MLE) with minimal engineering overhead. The core consists of 5 packages: mle-logging : Experiment logging with easy multi-seed and configuration aggregation. mle-hyperopt : Hyperparameter Optimization with config export, refinement & reloading. mle-monitor : Monitor cluster/cloud VM resource utilization & protocol experiments. mle-scheduler : Schedule & monitor jobs on Slurm, GridEngine clusters & GCP VMs. mle-toolbox : Glues everything together to manage & post-process experiments. Note I : A template repository of an infrastructure-based project can be found in the mle-project . You can inspect your experiment stack in an interactive web UI: mle-laboratory . Note II : mle-logging , mle-hyperopt , mle-monitor and mle-scheduler are standalone packages and can be used independently of the utilities provided by the mle-toolbox .","title":"Welcome to the MLE-Infrastructure \ud83d\udd2c"},{"location":"mle_hyperopt/","text":"The mle-hyperopt Package Hyperparameter optimization made easy \ud83d\ude80 The mle-hyperopt package provides the backbone for all search experiments in the mle-toolbox . It comes with a simple and intuitive API for hyperparameter optimization of your pipeline and will be called internally. Nonetheless, it is a standalone package which can simply be imported. It supports real, integer & categorical search variables and single- or multi-objective optimization. Core features include the following: API Simplicity : strategy.ask() , strategy.tell() interface & space definition. Strategy Diversity : Grid, random, coordinate search, SMBO & wrapping around FAIR's nevergrad . Search Space Refinement based on the top performing configs via strategy.refine(top_k=10) . Export of configurations to execute via e.g. python train.py --config_fname config.yaml . Storage & reload search logs via strategy.save(<log_fname>) , strategy.load(<log_fname>) . For a quickstart check out the notebook blog \ud83d\udcd6. The API \ud83c\udfae from mle_hyperopt import RandomSearch # Instantiate random search class strategy = RandomSearch ( real = { \"lrate\" : { \"begin\" : 0.1 , \"end\" : 0.5 , \"prior\" : \"log-uniform\" }}, integer = { \"batch_size\" : { \"begin\" : 32 , \"end\" : 128 , \"prior\" : \"uniform\" }}, categorical = { \"arch\" : [ \"mlp\" , \"cnn\" ]}) # Simple ask - eval - tell API configs = strategy . ask ( 5 ) values = [ train_network ( ** c ) for c in configs ] strategy . tell ( configs , values ) Implemented Search Types \ud83d\udd2d Search Type Description search_config GridSearch Search over list of discrete values - RandomSearch Random search over variable ranges refine_after , refine_top_k CoordinateSearch Coordinate-wise optimization with fixed defaults order , defaults SMBOSearch Sequential model-based optimization base_estimator , acq_function , n_initial_points NevergradSearch Multi-objective nevergrad wrapper optimizer , budget_size , num_workers Variable Types & Hyperparameter Spaces \ud83c\udf0d Variable Type Space Specification real Real-valued Dict : begin , end , prior / bins (grid) integer Integer-valued Dict : begin , end , prior / bins (grid) categorical Categorical List : Values to search over Installation \u23f3 A PyPI installation is available via: pip install mle-hyperopt Alternatively, you can clone this repository and afterwards 'manually' install it: git clone https://github.com/RobertTLange/mle-hyperopt.git cd mle-hyperopt pip install -e . Further Options \ud83d\udeb4 Saving & Reloading Logs \ud83c\udfea # Storing & reloading of results from .pkl strategy . save ( \"search_log.json\" ) strategy = RandomSearch ( ... , reload_path = \"search_log.json\" ) # Or manually add info after class instantiation strategy = RandomSearch ( ... ) strategy . load ( \"search_log.json\" ) Search Decorator \ud83e\uddf6 from mle_hyperopt import hyperopt @hyperopt ( strategy_type = \"grid\" , num_search_iters = 25 , real = { \"x\" : { \"begin\" : 0. , \"end\" : 0.5 , \"bins\" : 5 }, \"y\" : { \"begin\" : 0 , \"end\" : 0.5 , \"bins\" : 5 }}) def circle ( config ): distance = abs (( config [ \"x\" ] ** 2 + config [ \"y\" ] ** 2 )) return distance strategy = circle () Storing Configuration Files \ud83d\udcd1 # Store 2 proposed configurations - eval_0.yaml, eval_1.yaml strategy . ask ( 2 , store = True ) # Store with explicit configuration filenames - conf_0.yaml, conf_1.yaml strategy . ask ( 2 , store = True , config_fnames = [ \"conf_0.yaml\" , \"conf_1.yaml\" ]) Retrieving Top Performers & Visualizing Results \ud83d\udcc9 # Get the top k best performing configurations id , configs , values = strategy . get_best ( top_k = 4 ) # Plot timeseries of best performing score over search iterations strategy . plot_best () # Print out ranking of best performers strategy . print_ranking ( top_k = 3 ) Refining the Search Space of Your Strategy \ud83e\ude93 # Refine the search space after 5 & 10 iterations based on top 2 configurations strategy = RandomSearch ( real = { \"lrate\" : { \"begin\" : 0.1 , \"end\" : 0.5 , \"prior\" : \"uniform\" }}, integer = { \"batch_size\" : { \"begin\" : 1 , \"end\" : 5 , \"prior\" : \"log-uniform\" }}, categorical = { \"arch\" : [ \"mlp\" , \"cnn\" ]}, search_config = { \"refine_after\" : [ 5 , 10 ], \"refine_top_k\" : 2 }) # Or do so manually using `refine` method strategy . tell ( ... ) strategy . refine ( top_k = 2 ) Note that the search space refinement is only implemented for random, SMBO and nevergrad-based search strategies.","title":"The `mle-hyperopt` Package"},{"location":"mle_hyperopt/#the-mle-hyperopt-package","text":"","title":"The mle-hyperopt Package"},{"location":"mle_hyperopt/#hyperparameter-optimization-made-easy","text":"The mle-hyperopt package provides the backbone for all search experiments in the mle-toolbox . It comes with a simple and intuitive API for hyperparameter optimization of your pipeline and will be called internally. Nonetheless, it is a standalone package which can simply be imported. It supports real, integer & categorical search variables and single- or multi-objective optimization. Core features include the following: API Simplicity : strategy.ask() , strategy.tell() interface & space definition. Strategy Diversity : Grid, random, coordinate search, SMBO & wrapping around FAIR's nevergrad . Search Space Refinement based on the top performing configs via strategy.refine(top_k=10) . Export of configurations to execute via e.g. python train.py --config_fname config.yaml . Storage & reload search logs via strategy.save(<log_fname>) , strategy.load(<log_fname>) . For a quickstart check out the notebook blog \ud83d\udcd6.","title":"Hyperparameter optimization made easy \ud83d\ude80"},{"location":"mle_hyperopt/#the-api","text":"from mle_hyperopt import RandomSearch # Instantiate random search class strategy = RandomSearch ( real = { \"lrate\" : { \"begin\" : 0.1 , \"end\" : 0.5 , \"prior\" : \"log-uniform\" }}, integer = { \"batch_size\" : { \"begin\" : 32 , \"end\" : 128 , \"prior\" : \"uniform\" }}, categorical = { \"arch\" : [ \"mlp\" , \"cnn\" ]}) # Simple ask - eval - tell API configs = strategy . ask ( 5 ) values = [ train_network ( ** c ) for c in configs ] strategy . tell ( configs , values )","title":"The API \ud83c\udfae"},{"location":"mle_hyperopt/#implemented-search-types","text":"Search Type Description search_config GridSearch Search over list of discrete values - RandomSearch Random search over variable ranges refine_after , refine_top_k CoordinateSearch Coordinate-wise optimization with fixed defaults order , defaults SMBOSearch Sequential model-based optimization base_estimator , acq_function , n_initial_points NevergradSearch Multi-objective nevergrad wrapper optimizer , budget_size , num_workers","title":"Implemented Search Types    \ud83d\udd2d"},{"location":"mle_hyperopt/#variable-types-hyperparameter-spaces","text":"Variable Type Space Specification real Real-valued Dict : begin , end , prior / bins (grid) integer Integer-valued Dict : begin , end , prior / bins (grid) categorical Categorical List : Values to search over","title":"Variable Types &amp; Hyperparameter Spaces \ud83c\udf0d"},{"location":"mle_hyperopt/#installation","text":"A PyPI installation is available via: pip install mle-hyperopt Alternatively, you can clone this repository and afterwards 'manually' install it: git clone https://github.com/RobertTLange/mle-hyperopt.git cd mle-hyperopt pip install -e .","title":"Installation \u23f3"},{"location":"mle_hyperopt/#further-options","text":"","title":"Further Options \ud83d\udeb4"},{"location":"mle_hyperopt/#saving-reloading-logs","text":"# Storing & reloading of results from .pkl strategy . save ( \"search_log.json\" ) strategy = RandomSearch ( ... , reload_path = \"search_log.json\" ) # Or manually add info after class instantiation strategy = RandomSearch ( ... ) strategy . load ( \"search_log.json\" )","title":"Saving &amp; Reloading Logs \ud83c\udfea"},{"location":"mle_hyperopt/#search-decorator","text":"from mle_hyperopt import hyperopt @hyperopt ( strategy_type = \"grid\" , num_search_iters = 25 , real = { \"x\" : { \"begin\" : 0. , \"end\" : 0.5 , \"bins\" : 5 }, \"y\" : { \"begin\" : 0 , \"end\" : 0.5 , \"bins\" : 5 }}) def circle ( config ): distance = abs (( config [ \"x\" ] ** 2 + config [ \"y\" ] ** 2 )) return distance strategy = circle ()","title":"Search Decorator \ud83e\uddf6"},{"location":"mle_hyperopt/#storing-configuration-files","text":"# Store 2 proposed configurations - eval_0.yaml, eval_1.yaml strategy . ask ( 2 , store = True ) # Store with explicit configuration filenames - conf_0.yaml, conf_1.yaml strategy . ask ( 2 , store = True , config_fnames = [ \"conf_0.yaml\" , \"conf_1.yaml\" ])","title":"Storing Configuration Files \ud83d\udcd1"},{"location":"mle_hyperopt/#retrieving-top-performers-visualizing-results","text":"# Get the top k best performing configurations id , configs , values = strategy . get_best ( top_k = 4 ) # Plot timeseries of best performing score over search iterations strategy . plot_best () # Print out ranking of best performers strategy . print_ranking ( top_k = 3 )","title":"Retrieving Top Performers &amp; Visualizing Results \ud83d\udcc9"},{"location":"mle_hyperopt/#refining-the-search-space-of-your-strategy","text":"# Refine the search space after 5 & 10 iterations based on top 2 configurations strategy = RandomSearch ( real = { \"lrate\" : { \"begin\" : 0.1 , \"end\" : 0.5 , \"prior\" : \"uniform\" }}, integer = { \"batch_size\" : { \"begin\" : 1 , \"end\" : 5 , \"prior\" : \"log-uniform\" }}, categorical = { \"arch\" : [ \"mlp\" , \"cnn\" ]}, search_config = { \"refine_after\" : [ 5 , 10 ], \"refine_top_k\" : 2 }) # Or do so manually using `refine` method strategy . tell ( ... ) strategy . refine ( top_k = 2 ) Note that the search space refinement is only implemented for random, SMBO and nevergrad-based search strategies.","title":"Refining the Search Space of Your Strategy \ud83e\ude93"},{"location":"mle_logging/","text":"The mle-logging Package Experiment logging made easy \ud83d\udcd6 Each Python-based experiment is assumed to use a custom logger: The MLELogger . This enables the standardization needed to automatically aggregate multiple random runs and to log performance across hyperparameter searches. For a quickstart checkout the notebook blog \ud83d\ude80 The API \ud83c\udfae from mle_logging import MLELogger # Instantiate logging to experiment_dir log = MLELogger ( time_to_track = [ 'num_updates' , 'num_epochs' ], what_to_track = [ 'train_loss' , 'test_loss' ], experiment_dir = \"experiment_dir/\" , model_type = 'torch' ) time_tic = { 'num_updates' : 10 , 'num_epochs' : 1 } stats_tic = { 'train_loss' : 0.1234 , 'test_loss' : 0.1235 } # Update the log with collected data & save it to .hdf5 log . update ( time_tic , stats_tic ) log . save () You can also log model checkpoints, matplotlib figures and other .pkl compatible objects. # Save a model (torch, tensorflow, sklearn, jax, numpy) import torchvision.models as models model = models . resnet18 () log . save_model ( model ) # Save a matplotlib figure as .png fig , ax = plt . subplots () log . save_plot ( fig ) # You can also save (somewhat) arbitrary objects .pkl some_dict = { \"hi\" : \"there\" } log . save_extra ( some_dict ) Or do everything in a single line... log . update ( time_tic , stats_tic , model , fig , extra , save = True ) File Structure & Re-Loading \ud83d\udcda The MLELogger will create a nested directory, which looks as follows: experiment_dir \u251c\u2500\u2500 extra: Stores saved .pkl object files \u251c\u2500\u2500 figures: Stores saved .png figures \u251c\u2500\u2500 logs: Stores .hdf5 log files (meta, stats, time) \u251c\u2500\u2500 models: Stores different model checkpoints \u251c\u2500\u2500 init: Stores initial checkpoint \u251c\u2500\u2500 final: Stores most recent checkpoint \u251c\u2500\u2500 every_k: Stores every k-th checkpoint provided in update \u251c\u2500\u2500 top_k: Stores portfolio of top-k checkpoints based on performance \u251c\u2500\u2500 tboards: Stores tensorboards for model checkpointing \u251c\u2500\u2500 <config_name>.json: Copy of configuration file (if provided) For visualization and post-processing load the results via from mle_logging import load_log log_out = load_log ( \"experiment_dir/\" ) # The results can be accessed via meta, stats and time keys # >>> log_out.meta.keys() # odict_keys(['experiment_dir', 'extra_storage_paths', 'fig_storage_paths', 'log_paths', 'model_ckpt', 'model_type']) # >>> log_out.stats.keys() # odict_keys(['test_loss', 'train_loss']) # >>> log_out.time.keys() # odict_keys(['time', 'num_epochs', 'num_updates', 'time_elapsed']) If an experiment was aborted, you can reload and continue the previous run via the reload=True option: log = MLELogger ( time_to_track = [ 'num_updates' , 'num_epochs' ], what_to_track = [ 'train_loss' , 'test_loss' ], experiment_dir = \"experiment_dir/\" , model_type = 'torch' , reload = True ) Installation \u23f3 A PyPI installation is available via: pip install mle-logging Alternatively, you can clone this repository and afterwards 'manually' install it: git clone https://github.com/RobertTLange/mle-logging.git cd mle-logging pip install -e . Advanced Options \ud83d\udeb4 Merging Multiple Logs \ud83d\udc6b Merging Multiple Random Seeds \ud83c\udf31 + \ud83c\udf31 from mle_logging import merge_seed_logs merge_seed_logs ( \"multi_seed.hdf\" , \"experiment_dir/\" ) log_out = load_log ( \"experiment_dir/\" ) # >>> log.eval_ids # ['seed_1', 'seed_2'] Merging Multiple Configurations \ud83d\udd16 + \ud83d\udd16 from mle_logging import merge_config_logs , load_meta_log merge_config_logs ( experiment_dir = \"experiment_dir/\" , all_run_ids = [ \"config_1\" , \"config_2\" ]) meta_log = load_meta_log ( \"multi_config_dir/meta_log.hdf5\" ) # >>> log.eval_ids # ['config_2', 'config_1'] # >>> meta_log.config_1.stats.test_loss.keys() # odict_keys(['mean', 'std', 'p50', 'p10', 'p25', 'p75', 'p90'])) Plotting of Logs \ud83e\uddd1\u200d\ud83c\udfa8 meta_log = load_meta_log ( \"multi_config_dir/meta_log.hdf5\" ) meta_log . plot ( \"train_loss\" , \"num_updates\" ) Storing Checkpoint Portfolios \ud83d\udcc2 Logging every k-th checkpoint update \u2757 \u23e9 ... \u23e9 \u2757 # Save every second checkpoint provided in log.update (stored in models/every_k) log = MLELogger ( time_to_track = [ 'num_updates' , 'num_epochs' ], what_to_track = [ 'train_loss' , 'test_loss' ], experiment_dir = 'every_k_dir/' , model_type = 'torch' , ckpt_time_to_track = 'num_updates' , save_every_k_ckpt = 2 ) Logging top-k checkpoints based on metric \ud83d\udd31 # Save top-3 checkpoints provided in log.update (stored in models/top_k) # Based on minimizing the test_loss metric log = MLELogger ( time_to_track = [ 'num_updates' , 'num_epochs' ], what_to_track = [ 'train_loss' , 'test_loss' ], experiment_dir = \"top_k_dir/\" , model_type = 'torch' , ckpt_time_to_track = 'num_updates' , save_top_k_ckpt = 3 , top_k_metric_name = \"test_loss\" , top_k_minimize_metric = True )","title":"The `mle-logging` Package"},{"location":"mle_logging/#the-mle-logging-package","text":"","title":"The mle-logging Package"},{"location":"mle_logging/#experiment-logging-made-easy","text":"Each Python-based experiment is assumed to use a custom logger: The MLELogger . This enables the standardization needed to automatically aggregate multiple random runs and to log performance across hyperparameter searches. For a quickstart checkout the notebook blog \ud83d\ude80","title":"Experiment logging made easy \ud83d\udcd6"},{"location":"mle_logging/#the-api","text":"from mle_logging import MLELogger # Instantiate logging to experiment_dir log = MLELogger ( time_to_track = [ 'num_updates' , 'num_epochs' ], what_to_track = [ 'train_loss' , 'test_loss' ], experiment_dir = \"experiment_dir/\" , model_type = 'torch' ) time_tic = { 'num_updates' : 10 , 'num_epochs' : 1 } stats_tic = { 'train_loss' : 0.1234 , 'test_loss' : 0.1235 } # Update the log with collected data & save it to .hdf5 log . update ( time_tic , stats_tic ) log . save () You can also log model checkpoints, matplotlib figures and other .pkl compatible objects. # Save a model (torch, tensorflow, sklearn, jax, numpy) import torchvision.models as models model = models . resnet18 () log . save_model ( model ) # Save a matplotlib figure as .png fig , ax = plt . subplots () log . save_plot ( fig ) # You can also save (somewhat) arbitrary objects .pkl some_dict = { \"hi\" : \"there\" } log . save_extra ( some_dict ) Or do everything in a single line... log . update ( time_tic , stats_tic , model , fig , extra , save = True )","title":"The API \ud83c\udfae"},{"location":"mle_logging/#file-structure-re-loading","text":"The MLELogger will create a nested directory, which looks as follows: experiment_dir \u251c\u2500\u2500 extra: Stores saved .pkl object files \u251c\u2500\u2500 figures: Stores saved .png figures \u251c\u2500\u2500 logs: Stores .hdf5 log files (meta, stats, time) \u251c\u2500\u2500 models: Stores different model checkpoints \u251c\u2500\u2500 init: Stores initial checkpoint \u251c\u2500\u2500 final: Stores most recent checkpoint \u251c\u2500\u2500 every_k: Stores every k-th checkpoint provided in update \u251c\u2500\u2500 top_k: Stores portfolio of top-k checkpoints based on performance \u251c\u2500\u2500 tboards: Stores tensorboards for model checkpointing \u251c\u2500\u2500 <config_name>.json: Copy of configuration file (if provided) For visualization and post-processing load the results via from mle_logging import load_log log_out = load_log ( \"experiment_dir/\" ) # The results can be accessed via meta, stats and time keys # >>> log_out.meta.keys() # odict_keys(['experiment_dir', 'extra_storage_paths', 'fig_storage_paths', 'log_paths', 'model_ckpt', 'model_type']) # >>> log_out.stats.keys() # odict_keys(['test_loss', 'train_loss']) # >>> log_out.time.keys() # odict_keys(['time', 'num_epochs', 'num_updates', 'time_elapsed']) If an experiment was aborted, you can reload and continue the previous run via the reload=True option: log = MLELogger ( time_to_track = [ 'num_updates' , 'num_epochs' ], what_to_track = [ 'train_loss' , 'test_loss' ], experiment_dir = \"experiment_dir/\" , model_type = 'torch' , reload = True )","title":"File Structure &amp; Re-Loading \ud83d\udcda"},{"location":"mle_logging/#installation","text":"A PyPI installation is available via: pip install mle-logging Alternatively, you can clone this repository and afterwards 'manually' install it: git clone https://github.com/RobertTLange/mle-logging.git cd mle-logging pip install -e .","title":"Installation \u23f3"},{"location":"mle_logging/#advanced-options","text":"","title":"Advanced Options \ud83d\udeb4"},{"location":"mle_logging/#merging-multiple-logs","text":"Merging Multiple Random Seeds \ud83c\udf31 + \ud83c\udf31 from mle_logging import merge_seed_logs merge_seed_logs ( \"multi_seed.hdf\" , \"experiment_dir/\" ) log_out = load_log ( \"experiment_dir/\" ) # >>> log.eval_ids # ['seed_1', 'seed_2'] Merging Multiple Configurations \ud83d\udd16 + \ud83d\udd16 from mle_logging import merge_config_logs , load_meta_log merge_config_logs ( experiment_dir = \"experiment_dir/\" , all_run_ids = [ \"config_1\" , \"config_2\" ]) meta_log = load_meta_log ( \"multi_config_dir/meta_log.hdf5\" ) # >>> log.eval_ids # ['config_2', 'config_1'] # >>> meta_log.config_1.stats.test_loss.keys() # odict_keys(['mean', 'std', 'p50', 'p10', 'p25', 'p75', 'p90']))","title":"Merging Multiple Logs \ud83d\udc6b"},{"location":"mle_logging/#plotting-of-logs","text":"meta_log = load_meta_log ( \"multi_config_dir/meta_log.hdf5\" ) meta_log . plot ( \"train_loss\" , \"num_updates\" )","title":"Plotting of Logs \ud83e\uddd1\u200d\ud83c\udfa8"},{"location":"mle_logging/#storing-checkpoint-portfolios","text":"Logging every k-th checkpoint update \u2757 \u23e9 ... \u23e9 \u2757 # Save every second checkpoint provided in log.update (stored in models/every_k) log = MLELogger ( time_to_track = [ 'num_updates' , 'num_epochs' ], what_to_track = [ 'train_loss' , 'test_loss' ], experiment_dir = 'every_k_dir/' , model_type = 'torch' , ckpt_time_to_track = 'num_updates' , save_every_k_ckpt = 2 ) Logging top-k checkpoints based on metric \ud83d\udd31 # Save top-3 checkpoints provided in log.update (stored in models/top_k) # Based on minimizing the test_loss metric log = MLELogger ( time_to_track = [ 'num_updates' , 'num_epochs' ], what_to_track = [ 'train_loss' , 'test_loss' ], experiment_dir = \"top_k_dir/\" , model_type = 'torch' , ckpt_time_to_track = 'num_updates' , save_top_k_ckpt = 3 , top_k_metric_name = \"test_loss\" , top_k_minimize_metric = True )","title":"Storing Checkpoint Portfolios \ud83d\udcc2"},{"location":"mle_monitor/","text":"The mle-monitor Package Resource monitoring made easy \ud83d\udcfa \"Did I already run this experiment before? How many resources are currently available on my cluster?\" If these are common questions you encounter during your daily life as a researcher, then mle-monitor is made for you. It provides a lightweight API for tracking your experiments using a pickle protocol database (e.g. for hyperparameter searches and/or multi-configuration/multi-seed runs). Furthermore, it comes with built-in resource monitoring on Slurm/Grid Engine clusters and local machines/servers. mle-monitor provides three core functionalities: MLEProtocol : A composable protocol database API for ML experiments. MLEResource : A tool for obtaining server/cluster usage statistics. MLEDashboard : A dashboard visualizing resource usage & experiment protocol. To get started I recommend checking out the colab notebook and an example workflow . MLEProtocol : Keeping Track of Your Experiments \ud83d\udcdd from mle_monitor import MLEProtocol # Load protocol database or create new one -> print summary protocol_db = MLEProtocol ( \"mle_protocol.db\" , verbose = False ) protocol_db . summary ( tail = 10 , verbose = True ) # Draft data to store in protocol & add it to the protocol meta_data = { \"purpose\" : \"Grid search\" , # Purpose of experiment \"project_name\" : \"MNIST\" , # Project name of experiment \"experiment_type\" : \"hyperparameter-search\" , # Type of experiment \"experiment_dir\" : \"experiments/logs\" , # Experiment directory \"num_total_jobs\" : 10 , # Number of total jobs to run ... } new_experiment_id = protocol_db . add ( meta_data ) # ... train your 10 (pseudo) networks/complete respective jobs for i in range ( 10 ): protocol_db . update_progress_bar ( new_experiment_id ) # Wrap up an experiment (store completion time, etc.) protocol_db . complete ( new_experiment_id ) The meta data can contain the following keys: Search Type Description Default purpose Purpose of experiment 'None provided' project_name Project name of experiment 'default' exec_resource Resource jobs are run on 'local' experiment_dir Experiment log storage directory 'experiments' experiment_type Type of experiment to run 'single' base_fname Main code script to execute 'main.py' config_fname Config file path of experiment 'base_config.yaml' num_seeds Number of evaluations seeds 1 num_total_jobs Number of total jobs to run 1 num_job_batches Number of jobs in single batch 1 num_jobs_per_batch Number of sequential job batches 1 time_per_job Expected duration: days-hours-minutes '00:01:00' num_cpus Number of CPUs used in job 1 num_gpus Number of GPUs used in job 0 Additionally you can synchronize the protocol with a Google Cloud Storage (GCS) bucket by providing cloud_settings . In this case also the results stored in experiment_dir will be uploaded to the GCS bucket, when you call protocol.complete() . # Define GCS settings - requires 'GOOGLE_APPLICATION_CREDENTIALS' env var. cloud_settings = { \"project_name\" : \"mle-toolbox\" , # GCP project name \"bucket_name\" : \"mle-protocol\" , # GCS bucket name \"use_protocol_sync\" : True , # Whether to sync the protocol to GCS \"use_results_storage\" : True , # Whether to sync experiment_dir to GCS } protocol_db = MLEProtocol ( \"mle_protocol.db\" , cloud_settings , verbose = True ) The MLEResource : Keeping Track of Your Resources \ud83d\udcc9 On Your Local Machine from mle_monitor import MLEResource # Instantiate local resource and get usage data resource = MLEResource ( resource_name = \"local\" ) resource_data = resource . monitor () On a Slurm Cluster resource = MLEResource ( resource_name = \"slurm-cluster\" , monitor_config = { \"partitions\" : [ \"<partition-1>\" , \"<partition-2>\" ]}, ) On a Grid Engine Cluster resource = MLEResource ( resource_name = \"sge-cluster\" , monitor_config = { \"queues\" : [ \"<queue-1>\" , \"<queue-2>\" ]} ) The MLEDashboard : Dashboard Visualization \ud83c\udf9e\ufe0f from mle_monitor import MLEDashboard # Instantiate dashboard with protocol and resource dashboard = MLEDashboard ( protocol , resource ) # Get a static snapshot of the protocol & resource utilisation printed in console dashboard . snapshot () # Run monitoring in while loop - dashboard dashboard . live () Installation \u23f3 A PyPI installation is available via: pip install mle-monitor Alternatively, you can clone this repository and afterwards 'manually' install it: git clone https://github.com/mle-infrastructure/mle-monitor.git cd mle-monitor pip install -e .","title":"The `mle-monitor` Package"},{"location":"mle_monitor/#the-mle-monitor-package","text":"","title":"The mle-monitor Package"},{"location":"mle_monitor/#resource-monitoring-made-easy","text":"\"Did I already run this experiment before? How many resources are currently available on my cluster?\" If these are common questions you encounter during your daily life as a researcher, then mle-monitor is made for you. It provides a lightweight API for tracking your experiments using a pickle protocol database (e.g. for hyperparameter searches and/or multi-configuration/multi-seed runs). Furthermore, it comes with built-in resource monitoring on Slurm/Grid Engine clusters and local machines/servers. mle-monitor provides three core functionalities: MLEProtocol : A composable protocol database API for ML experiments. MLEResource : A tool for obtaining server/cluster usage statistics. MLEDashboard : A dashboard visualizing resource usage & experiment protocol. To get started I recommend checking out the colab notebook and an example workflow .","title":"Resource monitoring made easy \ud83d\udcfa"},{"location":"mle_monitor/#mleprotocol-keeping-track-of-your-experiments","text":"from mle_monitor import MLEProtocol # Load protocol database or create new one -> print summary protocol_db = MLEProtocol ( \"mle_protocol.db\" , verbose = False ) protocol_db . summary ( tail = 10 , verbose = True ) # Draft data to store in protocol & add it to the protocol meta_data = { \"purpose\" : \"Grid search\" , # Purpose of experiment \"project_name\" : \"MNIST\" , # Project name of experiment \"experiment_type\" : \"hyperparameter-search\" , # Type of experiment \"experiment_dir\" : \"experiments/logs\" , # Experiment directory \"num_total_jobs\" : 10 , # Number of total jobs to run ... } new_experiment_id = protocol_db . add ( meta_data ) # ... train your 10 (pseudo) networks/complete respective jobs for i in range ( 10 ): protocol_db . update_progress_bar ( new_experiment_id ) # Wrap up an experiment (store completion time, etc.) protocol_db . complete ( new_experiment_id ) The meta data can contain the following keys: Search Type Description Default purpose Purpose of experiment 'None provided' project_name Project name of experiment 'default' exec_resource Resource jobs are run on 'local' experiment_dir Experiment log storage directory 'experiments' experiment_type Type of experiment to run 'single' base_fname Main code script to execute 'main.py' config_fname Config file path of experiment 'base_config.yaml' num_seeds Number of evaluations seeds 1 num_total_jobs Number of total jobs to run 1 num_job_batches Number of jobs in single batch 1 num_jobs_per_batch Number of sequential job batches 1 time_per_job Expected duration: days-hours-minutes '00:01:00' num_cpus Number of CPUs used in job 1 num_gpus Number of GPUs used in job 0 Additionally you can synchronize the protocol with a Google Cloud Storage (GCS) bucket by providing cloud_settings . In this case also the results stored in experiment_dir will be uploaded to the GCS bucket, when you call protocol.complete() . # Define GCS settings - requires 'GOOGLE_APPLICATION_CREDENTIALS' env var. cloud_settings = { \"project_name\" : \"mle-toolbox\" , # GCP project name \"bucket_name\" : \"mle-protocol\" , # GCS bucket name \"use_protocol_sync\" : True , # Whether to sync the protocol to GCS \"use_results_storage\" : True , # Whether to sync experiment_dir to GCS } protocol_db = MLEProtocol ( \"mle_protocol.db\" , cloud_settings , verbose = True )","title":"MLEProtocol: Keeping Track of Your Experiments \ud83d\udcdd"},{"location":"mle_monitor/#the-mleresource-keeping-track-of-your-resources","text":"","title":"The MLEResource: Keeping Track of Your Resources \ud83d\udcc9"},{"location":"mle_monitor/#on-your-local-machine","text":"from mle_monitor import MLEResource # Instantiate local resource and get usage data resource = MLEResource ( resource_name = \"local\" ) resource_data = resource . monitor ()","title":"On Your Local Machine"},{"location":"mle_monitor/#on-a-slurm-cluster","text":"resource = MLEResource ( resource_name = \"slurm-cluster\" , monitor_config = { \"partitions\" : [ \"<partition-1>\" , \"<partition-2>\" ]}, )","title":"On a Slurm Cluster"},{"location":"mle_monitor/#on-a-grid-engine-cluster","text":"resource = MLEResource ( resource_name = \"sge-cluster\" , monitor_config = { \"queues\" : [ \"<queue-1>\" , \"<queue-2>\" ]} )","title":"On a Grid Engine Cluster"},{"location":"mle_monitor/#the-mledashboard-dashboard-visualization","text":"from mle_monitor import MLEDashboard # Instantiate dashboard with protocol and resource dashboard = MLEDashboard ( protocol , resource ) # Get a static snapshot of the protocol & resource utilisation printed in console dashboard . snapshot () # Run monitoring in while loop - dashboard dashboard . live ()","title":"The MLEDashboard: Dashboard Visualization \ud83c\udf9e\ufe0f"},{"location":"mle_monitor/#installation","text":"A PyPI installation is available via: pip install mle-monitor Alternatively, you can clone this repository and afterwards 'manually' install it: git clone https://github.com/mle-infrastructure/mle-monitor.git cd mle-monitor pip install -e .","title":"Installation \u23f3"},{"location":"mle_scheduler/","text":"The mle-scheduler Package Resource allocation made easy \ud83d\ude82 mle-scheduler provides a lightweight API to launch and monitor job queues on Slurm / Open Grid Engine clusters, SSH servers or Google Cloud Platform VMs . It smoothly orchestrates simultaneous runs for different configurations and/or random seeds. It is meant to reduce boilerplate and to make job resource specification intuitive. It comes with two core pillars: MLEJob : Launches and monitors a single job on a resource (Slurm, Open Grid Engine, GCP, SSH, etc.). MLEQueue : Launches and monitors a queue of jobs with different training configurations and/or seeds. For a quickstart check out the notebook blog or the example scripts \ud83d\udcd6 Local Slurm Grid Engine SSH GCP Installation \u23f3 pip install mle-scheduler Managing a Single Job with MLEJob Locally \ud83d\ude80 from mle_scheduler import MLEJob # python train.py -config base_config_1.yaml -exp_dir logs_single -seed_id 1 job = MLEJob ( resource_to_run = \"local\" , job_filename = \"train.py\" , config_filename = \"base_config_1.yaml\" , experiment_dir = \"logs_single\" , seed_id = 1 ) _ = job . run () Managing a Queue of Jobs with MLEQueue Locally \ud83d\ude80...\ud83d\ude80 from mle_scheduler import MLEQueue # python train.py -config base_config_1.yaml -seed 0 -exp_dir logs_queue/<date>_base_config_1 # python train.py -config base_config_1.yaml -seed 1 -exp_dir logs_queue/<date>_base_config_1 # python train.py -config base_config_2.yaml -seed 0 -exp_dir logs_queue/<date>_base_config_2 # python train.py -config base_config_2.yaml -seed 1 -exp_dir logs_queue/<date>_base_config_2 queue = MLEQueue ( resource_to_run = \"local\" , job_filename = \"train.py\" , config_filenames = [ \"base_config_1.yaml\" , \"base_config_2.yaml\" ], random_seeds = [ 0 , 1 ], experiment_dir = \"logs_queue\" ) queue . run () Launching Slurm Cluster-Based Jobs \ud83d\udc12 # Each job requests 5 CPU cores & 1 V100S GPU & loads CUDA 10.0 job_args = { \"partition\" : \"<SLURM_PARTITION>\" , # Partition to schedule jobs on \"env_name\" : \"mle-toolbox\" , # Env to activate at job start-up \"use_conda_venv\" : True , # Whether to use anaconda venv \"num_logical_cores\" : 5 , # Number of requested CPU cores per job \"num_gpus\" : 1 , # Number of requested GPUs per job \"gpu_type\" : \"V100S\" , # GPU model requested for each job \"modules_to_load\" : \"nvidia/cuda/10.0\" # Modules to load at start-up } queue = MLEQueue ( resource_to_run = \"slurm-cluster\" , job_filename = \"train.py\" , job_arguments = job_args , config_filenames = [ \"base_config_1.yaml\" , \"base_config_2.yaml\" ], experiment_dir = \"logs_slurm\" , random_seeds = [ 0 , 1 ] ) queue . run () Launching GridEngine Cluster-Based Jobs \ud83d\udc18 # Each job requests 5 CPU cores & 1 V100S GPU w. CUDA 10.0 loaded job_args = { \"queue\" : \"<GRID_ENGINE_QUEUE>\" , # Queue to schedule jobs on \"env_name\" : \"mle-toolbox\" , # Env to activate at job start-up \"use_conda_venv\" : True , # Whether to use anaconda venv \"num_logical_cores\" : 5 , # Number of requested CPU cores per job \"num_gpus\" : 1 , # Number of requested GPUs per job \"gpu_type\" : \"V100S\" , # GPU model requested for each job \"gpu_prefix\" : \"cuda\" #$ -l {gpu_prefix}=\"{num_gpus}\" } queue = MLEQueue ( resource_to_run = \"slurm-cluster\" , job_filename = \"train.py\" , job_arguments = job_args , config_filenames = [ \"base_config_1.yaml\" , \"base_config_2.yaml\" ], experiment_dir = \"logs_grid_engine\" , random_seeds = [ 0 , 1 ] ) queue . run () Launching SSH Server-Based Jobs \ud83e\udd8a ssh_settings = { \"user_name\" : \"<SSH_USER_NAME>\" , # SSH server user name \"pkey_path\" : \"<PKEY_PATH>\" , # Private key path (e.g. ~/.ssh/id_rsa) \"main_server\" : \"<SSH_SERVER>\" , # SSH Server address \"jump_server\" : '' , # Jump host address \"ssh_port\" : 22 , # SSH port \"remote_dir\" : \"mle-code-dir\" , # Dir to sync code to on server \"start_up_copy_dir\" : True , # Whether to copy code to server \"clean_up_remote_dir\" : True # Whether to delete remote_dir on exit } job_args = { \"env_name\" : \"mle-toolbox\" , # Env to activate at job start-up \"use_conda_venv\" : True # Whether to use anaconda venv } queue = MLEQueue ( resource_to_run = \"ssh-node\" , job_filename = \"train.py\" , config_filenames = [ \"base_config_1.yaml\" , \"base_config_2.yaml\" ], random_seeds = [ 0 , 1 ], experiment_dir = \"logs_ssh_queue\" , job_arguments = job_args , ssh_settings = ssh_settings ) queue . run () Launching GCP VM-Based Jobs \ud83e\udd84 cloud_settings = { \"project_name\" : \"<GCP_PROJECT_NAME>\" , # Name of your GCP project \"bucket_name\" : \"<GCS_BUCKET_NAME>\" , # Name of your GCS bucket \"remote_dir\" : \"<GCS_CODE_DIR_NAME>\" , # Name of code dir in bucket \"start_up_copy_dir\" : True , # Whether to copy code to bucket \"clean_up_remote_dir\" : True # Whether to delete remote_dir on exit } job_args = { \"num_gpus\" : 0 , # Number of requested GPUs per job \"gpu_type\" : None , # GPU requested e.g. \"nvidia-tesla-v100\" \"num_logical_cores\" : 1 , # Number of requested CPU cores per job } queue = MLEQueue ( resource_to_run = \"gcp-cloud\" , job_filename = \"train.py\" , config_filenames = [ \"base_config_1.yaml\" , \"base_config_2.yaml\" ], random_seeds = [ 0 , 1 ], experiment_dir = \"logs_gcp_queue\" , job_arguments = job_args , cloud_settings = cloud_settings , ) queue . run ()","title":"The `mle-scheduler` Package"},{"location":"mle_scheduler/#the-mle-scheduler-package","text":"","title":"The mle-scheduler Package"},{"location":"mle_scheduler/#resource-allocation-made-easy","text":"mle-scheduler provides a lightweight API to launch and monitor job queues on Slurm / Open Grid Engine clusters, SSH servers or Google Cloud Platform VMs . It smoothly orchestrates simultaneous runs for different configurations and/or random seeds. It is meant to reduce boilerplate and to make job resource specification intuitive. It comes with two core pillars: MLEJob : Launches and monitors a single job on a resource (Slurm, Open Grid Engine, GCP, SSH, etc.). MLEQueue : Launches and monitors a queue of jobs with different training configurations and/or seeds. For a quickstart check out the notebook blog or the example scripts \ud83d\udcd6 Local Slurm Grid Engine SSH GCP","title":"Resource allocation made easy \ud83d\ude82"},{"location":"mle_scheduler/#installation","text":"pip install mle-scheduler","title":"Installation \u23f3"},{"location":"mle_scheduler/#managing-a-single-job-with-mlejob-locally","text":"from mle_scheduler import MLEJob # python train.py -config base_config_1.yaml -exp_dir logs_single -seed_id 1 job = MLEJob ( resource_to_run = \"local\" , job_filename = \"train.py\" , config_filename = \"base_config_1.yaml\" , experiment_dir = \"logs_single\" , seed_id = 1 ) _ = job . run ()","title":"Managing a Single Job with MLEJob Locally \ud83d\ude80"},{"location":"mle_scheduler/#managing-a-queue-of-jobs-with-mlequeue-locally","text":"from mle_scheduler import MLEQueue # python train.py -config base_config_1.yaml -seed 0 -exp_dir logs_queue/<date>_base_config_1 # python train.py -config base_config_1.yaml -seed 1 -exp_dir logs_queue/<date>_base_config_1 # python train.py -config base_config_2.yaml -seed 0 -exp_dir logs_queue/<date>_base_config_2 # python train.py -config base_config_2.yaml -seed 1 -exp_dir logs_queue/<date>_base_config_2 queue = MLEQueue ( resource_to_run = \"local\" , job_filename = \"train.py\" , config_filenames = [ \"base_config_1.yaml\" , \"base_config_2.yaml\" ], random_seeds = [ 0 , 1 ], experiment_dir = \"logs_queue\" ) queue . run ()","title":"Managing a Queue of Jobs with MLEQueue Locally \ud83d\ude80...\ud83d\ude80"},{"location":"mle_scheduler/#launching-slurm-cluster-based-jobs","text":"# Each job requests 5 CPU cores & 1 V100S GPU & loads CUDA 10.0 job_args = { \"partition\" : \"<SLURM_PARTITION>\" , # Partition to schedule jobs on \"env_name\" : \"mle-toolbox\" , # Env to activate at job start-up \"use_conda_venv\" : True , # Whether to use anaconda venv \"num_logical_cores\" : 5 , # Number of requested CPU cores per job \"num_gpus\" : 1 , # Number of requested GPUs per job \"gpu_type\" : \"V100S\" , # GPU model requested for each job \"modules_to_load\" : \"nvidia/cuda/10.0\" # Modules to load at start-up } queue = MLEQueue ( resource_to_run = \"slurm-cluster\" , job_filename = \"train.py\" , job_arguments = job_args , config_filenames = [ \"base_config_1.yaml\" , \"base_config_2.yaml\" ], experiment_dir = \"logs_slurm\" , random_seeds = [ 0 , 1 ] ) queue . run ()","title":"Launching Slurm Cluster-Based Jobs \ud83d\udc12"},{"location":"mle_scheduler/#launching-gridengine-cluster-based-jobs","text":"# Each job requests 5 CPU cores & 1 V100S GPU w. CUDA 10.0 loaded job_args = { \"queue\" : \"<GRID_ENGINE_QUEUE>\" , # Queue to schedule jobs on \"env_name\" : \"mle-toolbox\" , # Env to activate at job start-up \"use_conda_venv\" : True , # Whether to use anaconda venv \"num_logical_cores\" : 5 , # Number of requested CPU cores per job \"num_gpus\" : 1 , # Number of requested GPUs per job \"gpu_type\" : \"V100S\" , # GPU model requested for each job \"gpu_prefix\" : \"cuda\" #$ -l {gpu_prefix}=\"{num_gpus}\" } queue = MLEQueue ( resource_to_run = \"slurm-cluster\" , job_filename = \"train.py\" , job_arguments = job_args , config_filenames = [ \"base_config_1.yaml\" , \"base_config_2.yaml\" ], experiment_dir = \"logs_grid_engine\" , random_seeds = [ 0 , 1 ] ) queue . run ()","title":"Launching GridEngine Cluster-Based Jobs \ud83d\udc18"},{"location":"mle_scheduler/#launching-ssh-server-based-jobs","text":"ssh_settings = { \"user_name\" : \"<SSH_USER_NAME>\" , # SSH server user name \"pkey_path\" : \"<PKEY_PATH>\" , # Private key path (e.g. ~/.ssh/id_rsa) \"main_server\" : \"<SSH_SERVER>\" , # SSH Server address \"jump_server\" : '' , # Jump host address \"ssh_port\" : 22 , # SSH port \"remote_dir\" : \"mle-code-dir\" , # Dir to sync code to on server \"start_up_copy_dir\" : True , # Whether to copy code to server \"clean_up_remote_dir\" : True # Whether to delete remote_dir on exit } job_args = { \"env_name\" : \"mle-toolbox\" , # Env to activate at job start-up \"use_conda_venv\" : True # Whether to use anaconda venv } queue = MLEQueue ( resource_to_run = \"ssh-node\" , job_filename = \"train.py\" , config_filenames = [ \"base_config_1.yaml\" , \"base_config_2.yaml\" ], random_seeds = [ 0 , 1 ], experiment_dir = \"logs_ssh_queue\" , job_arguments = job_args , ssh_settings = ssh_settings ) queue . run ()","title":"Launching SSH Server-Based Jobs \ud83e\udd8a"},{"location":"mle_scheduler/#launching-gcp-vm-based-jobs","text":"cloud_settings = { \"project_name\" : \"<GCP_PROJECT_NAME>\" , # Name of your GCP project \"bucket_name\" : \"<GCS_BUCKET_NAME>\" , # Name of your GCS bucket \"remote_dir\" : \"<GCS_CODE_DIR_NAME>\" , # Name of code dir in bucket \"start_up_copy_dir\" : True , # Whether to copy code to bucket \"clean_up_remote_dir\" : True # Whether to delete remote_dir on exit } job_args = { \"num_gpus\" : 0 , # Number of requested GPUs per job \"gpu_type\" : None , # GPU requested e.g. \"nvidia-tesla-v100\" \"num_logical_cores\" : 1 , # Number of requested CPU cores per job } queue = MLEQueue ( resource_to_run = \"gcp-cloud\" , job_filename = \"train.py\" , config_filenames = [ \"base_config_1.yaml\" , \"base_config_2.yaml\" ], random_seeds = [ 0 , 1 ], experiment_dir = \"logs_gcp_queue\" , job_arguments = job_args , cloud_settings = cloud_settings , ) queue . run ()","title":"Launching GCP VM-Based Jobs \ud83e\udd84"},{"location":"dev/future_plans/","text":"The Future of the Toolbox You can find a couple things that need to be tackled in the issues of this project . Below is a quick overview of large milestones that could need your help: Make mle init beautiful/a smoother/more minimal experience. Better documentation via sphinx, code style and PEP setup. Automated env/container generation + clean up (delete if existing) Asynchronous job scheduling based on \"trigger event\". Core functionalities for Population-based training. Exploit Strategies Explore Strategies PBT_Manager Multi-objective SMBO (pareto front improvements). Based BOTorch with different acquisition functions. Make BaseHyperOptimisation more general/adaptive. Get rid of scikit-optimize unstable dependency. Modular adding of remote cloud VM backends: Google Cloud Platform VM instances Amazon Web Services Microsoft Azure mle-labortaory Web UI/Server. Based on streamlit Monitoring from everywhere with password protection Easy retrieval of results via click Easy report generation via click Launch experiments from UI interface More tests in test suite for core features of toolbox. Update existing integration tests Test the MLE_Logger Test log merging","title":"The Future of the Toolbox"},{"location":"dev/future_plans/#the-future-of-the-toolbox","text":"You can find a couple things that need to be tackled in the issues of this project . Below is a quick overview of large milestones that could need your help: Make mle init beautiful/a smoother/more minimal experience. Better documentation via sphinx, code style and PEP setup. Automated env/container generation + clean up (delete if existing) Asynchronous job scheduling based on \"trigger event\". Core functionalities for Population-based training. Exploit Strategies Explore Strategies PBT_Manager Multi-objective SMBO (pareto front improvements). Based BOTorch with different acquisition functions. Make BaseHyperOptimisation more general/adaptive. Get rid of scikit-optimize unstable dependency. Modular adding of remote cloud VM backends: Google Cloud Platform VM instances Amazon Web Services Microsoft Azure mle-labortaory Web UI/Server. Based on streamlit Monitoring from everywhere with password protection Easy retrieval of results via click Easy report generation via click Launch experiments from UI interface More tests in test suite for core features of toolbox. Update existing integration tests Test the MLE_Logger Test log merging","title":"The Future of the Toolbox"},{"location":"dev/infrastructure/","text":"Toolbox Infrastructure In this document you can learn everything about how to run experiments with the mle-toolbox . The mle-toolbox allows you to run different types of experiments locally or on an SGE cluster. You have to provide three inputs: An experiment/meta configuration .yaml file. A job configuration .json file. A python .py script that runs your training loop. The only things you have to do is specify your desired experiment. The toolbox automatically detects whether you start an experiment with access to multiple compute nodes. train.py takes three arguments: -config , -seed , -exp_dir This includes the standard inputs to the training function ( model_config , train_config , log_config ) but can be otherwise generalised to your applications. Jobs, Evals and Experiments Throughout the toolbox we refer to different granularities of compute loads. It helps being familiar with what these refer to (from lowest to highest level of specification): job : A single submission process on resource (e.g. one seed for one configuration) eval : A single parameter configuration which can be executed/trained for multiple seeds (individual jobs!) experiment : An entire sequence of jobs to be executed (e.g. grid search with pre/post-processing) Protocol DB Logging TBC Rich Dashboard Monitoring TBC Experiment Report Summarization TBC","title":"Toolbox Infrastructure"},{"location":"dev/infrastructure/#toolbox-infrastructure","text":"In this document you can learn everything about how to run experiments with the mle-toolbox . The mle-toolbox allows you to run different types of experiments locally or on an SGE cluster. You have to provide three inputs: An experiment/meta configuration .yaml file. A job configuration .json file. A python .py script that runs your training loop. The only things you have to do is specify your desired experiment. The toolbox automatically detects whether you start an experiment with access to multiple compute nodes. train.py takes three arguments: -config , -seed , -exp_dir This includes the standard inputs to the training function ( model_config , train_config , log_config ) but can be otherwise generalised to your applications.","title":"Toolbox Infrastructure"},{"location":"dev/infrastructure/#jobs-evals-and-experiments","text":"Throughout the toolbox we refer to different granularities of compute loads. It helps being familiar with what these refer to (from lowest to highest level of specification): job : A single submission process on resource (e.g. one seed for one configuration) eval : A single parameter configuration which can be executed/trained for multiple seeds (individual jobs!) experiment : An entire sequence of jobs to be executed (e.g. grid search with pre/post-processing)","title":"Jobs, Evals and Experiments"},{"location":"dev/infrastructure/#protocol-db-logging","text":"TBC","title":"Protocol DB Logging"},{"location":"dev/infrastructure/#rich-dashboard-monitoring","text":"TBC","title":"Rich Dashboard Monitoring"},{"location":"dev/infrastructure/#experiment-report-summarization","text":"TBC","title":"Experiment Report Summarization"},{"location":"dev/notes/","text":"Notes for Development Toolbox Philosophy Notes Biology protocol: Simply a recipe, or written design, for performing the experiment. Purpose: Formal statement which encompasses your tested hypothesis. Materials: What are major items needed to carry out your experiment? -> Git commit hash of code repository Methods: How will you set up your experiment? -> Hash of base_config.json -> Hash of meta_config.yaml Controls: What are you going to compare your results with? -> Other experiment id or None if no direct comparison Data Interpretation: What will be done with the data once it is collected? -> Generate figures from the experiment results Causality tools from Econometrics: Let's be scientific about assessing the impact of algorithmic modifications and performance comparisons. Multiple testing corrections Difference-in-difference estimation - experiment class Power assessment and p-value computation with automatic seed recommendation Notes for documentation Apply for 1000$ GCP credits: https://edu.google.com/programs/credits/research/?modal_active=none Sun Grid Engine Details on how to submit jobs with qsub More notes on the SGE system Slurm Note that slurm cluster head node allows you to only run 128 processes parallel Need to salloc into a node - figure out max running time! salloc --job-name \"InteractiveJob\" --cpus-per-task 8 --mem-per-cpu 1500 --time 20:00:00 --partition standard git seems to be not working. Remote connection?! add export var to .bashrc On Slurm it can make sense to start up a job for the experiment management in a screen/tmux session for monitoring of many jobs: screen srun --job-name \"InteractiveJob\" --cpus-per-task 1 --mem-per-cpu 1500 --time 01:00:00 --partition standard --pty bash Google Cloud Storage Checkout Google Storage Python API: https://googleapis.dev/python/storage/latest/blobs.html How to use gcloud with a proxy server https://stackoverflow.com/questions/43926668/python3-bigquery-or-google-cloud-python-through-http-proxy/43945207#43945207 How to set up Google Cloud Storage of the experiment log files Create a new project Create a new bucket in the project Set up an authentication key: https://cloud.google.com/docs/authentication/production#passing_variable pip install google-cloud-storage and export the Google authentification bath in your .bashrc script Add credentials path to cluster_config.py file & add the project + bucket name Set option whether entire experiment/figure directory should be stored! Toolbox Dependencies Pickle DB docs: https://patx.github.io/pickledb/commands.html Where to run examples & tests from Examples from the mle_toolbox/examples/ directory. Tests from the mle_toolbox/ directory Update docs homepage https://squidfunk.github.io/mkdocs-material/creating-your-site/ https://github.com/squidfunk/mkdocs-material pip install mkdocs-material mkdocs serve mkdocs gh-deploy --force GitHub Actions Billing: https://docs.github.com/en/billing/managing-billing-for-github-actions/about-billing-for-github-actions Nice visualization tools https://favicon.io/favicon-generator/ - For homepage MLE icon https://carbon.now.sh/ - for code screenshots https://github.com/homeport/termshot - for terminal output screenshots","title":"Notes for Development"},{"location":"dev/notes/#notes-for-development","text":"","title":"Notes for Development"},{"location":"dev/notes/#toolbox-philosophy-notes","text":"Biology protocol: Simply a recipe, or written design, for performing the experiment. Purpose: Formal statement which encompasses your tested hypothesis. Materials: What are major items needed to carry out your experiment? -> Git commit hash of code repository Methods: How will you set up your experiment? -> Hash of base_config.json -> Hash of meta_config.yaml Controls: What are you going to compare your results with? -> Other experiment id or None if no direct comparison Data Interpretation: What will be done with the data once it is collected? -> Generate figures from the experiment results Causality tools from Econometrics: Let's be scientific about assessing the impact of algorithmic modifications and performance comparisons. Multiple testing corrections Difference-in-difference estimation - experiment class Power assessment and p-value computation with automatic seed recommendation","title":"Toolbox Philosophy Notes"},{"location":"dev/notes/#notes-for-documentation","text":"Apply for 1000$ GCP credits: https://edu.google.com/programs/credits/research/?modal_active=none","title":"Notes for documentation"},{"location":"dev/notes/#sun-grid-engine","text":"Details on how to submit jobs with qsub More notes on the SGE system","title":"Sun Grid Engine"},{"location":"dev/notes/#slurm","text":"Note that slurm cluster head node allows you to only run 128 processes parallel Need to salloc into a node - figure out max running time! salloc --job-name \"InteractiveJob\" --cpus-per-task 8 --mem-per-cpu 1500 --time 20:00:00 --partition standard git seems to be not working. Remote connection?! add export var to .bashrc On Slurm it can make sense to start up a job for the experiment management in a screen/tmux session for monitoring of many jobs: screen srun --job-name \"InteractiveJob\" --cpus-per-task 1 --mem-per-cpu 1500 --time 01:00:00 --partition standard --pty bash","title":"Slurm"},{"location":"dev/notes/#google-cloud-storage","text":"Checkout Google Storage Python API: https://googleapis.dev/python/storage/latest/blobs.html How to use gcloud with a proxy server https://stackoverflow.com/questions/43926668/python3-bigquery-or-google-cloud-python-through-http-proxy/43945207#43945207 How to set up Google Cloud Storage of the experiment log files Create a new project Create a new bucket in the project Set up an authentication key: https://cloud.google.com/docs/authentication/production#passing_variable pip install google-cloud-storage and export the Google authentification bath in your .bashrc script Add credentials path to cluster_config.py file & add the project + bucket name Set option whether entire experiment/figure directory should be stored!","title":"Google Cloud Storage"},{"location":"dev/notes/#toolbox-dependencies","text":"Pickle DB docs: https://patx.github.io/pickledb/commands.html","title":"Toolbox Dependencies"},{"location":"dev/notes/#where-to-run-examples-tests-from","text":"Examples from the mle_toolbox/examples/ directory. Tests from the mle_toolbox/ directory","title":"Where to run examples &amp; tests from"},{"location":"dev/notes/#update-docs-homepage","text":"https://squidfunk.github.io/mkdocs-material/creating-your-site/ https://github.com/squidfunk/mkdocs-material pip install mkdocs-material mkdocs serve mkdocs gh-deploy --force","title":"Update docs homepage"},{"location":"dev/notes/#github-actions","text":"Billing: https://docs.github.com/en/billing/managing-billing-for-github-actions/about-billing-for-github-actions","title":"GitHub Actions"},{"location":"dev/notes/#nice-visualization-tools","text":"https://favicon.io/favicon-generator/ - For homepage MLE icon https://carbon.now.sh/ - for code screenshots https://github.com/homeport/termshot - for terminal output screenshots","title":"Nice visualization tools"},{"location":"dev/remote_backends/","text":"Adding Remote Backends","title":"Adding Remote Backends"},{"location":"dev/remote_backends/#adding-remote-backends","text":"","title":"Adding Remote Backends"},{"location":"dev/testing/","text":"Running The Test Suite flake8 Linting flake8 ./mle_toolbox --count --select=E9,F63,F7,F82 --show-source --statistics flake8 ./mle_toolbox --count --exit-zero --max-line-length=127 --statistics mypy Type Checking mypy mle_toolbox/. black Formatting black mle_toolbox/. --verbose Test Coverage Note : This page and content is still work in progress! Unit Tests pytest -vv tests/unit File loading: tests/unit/test_load_files.py .yaml experiment configuration .json run configuration meta_log.hdf5 hyper_log.pkl Trained models .pkl -based (sklearn) .npy -based (JAX) .pt -based (PyTorch) Experiment launch configuration file generation .qsub .sbash GCP-startup .sh file Logging Individual run assert key errors directory creation - correct file structure are files stored in correct location? is data correctly stored? Merging into meta_log.hdf5 Summary into hyper_log.pkl Reloading correctly All data types supported Tensorboard support Image storage support Experiment Protocol Logging Adding a new experiment Deleting a failed experiment Integration Tests pytest -vv tests/integration Experiment types running on different resources Single configuration: tests/integration/test_single_config.py Multiple configuration: tests/integration/test_multi_configs.py A/Synchronous Grid search experiment: tests/integration/test_grid_search.py Random search experiment: tests/integration/test_random_search.py SMBO search experiment: tests/integration/test_smbo_search.py PBT experiment Report generation Figure generation from meta-log Figure generation from hyper-log .md generation Results retrieval From GCS bucket From remote resource Toolbox initialization Config file changing Encryption ssh credentials GCS integration Pull dummy protocol DB Send results/retrieve results","title":"Running The Test Suite"},{"location":"dev/testing/#running-the-test-suite","text":"","title":"Running The Test Suite"},{"location":"dev/testing/#flake8-linting","text":"flake8 ./mle_toolbox --count --select=E9,F63,F7,F82 --show-source --statistics flake8 ./mle_toolbox --count --exit-zero --max-line-length=127 --statistics","title":"flake8 Linting"},{"location":"dev/testing/#mypy-type-checking","text":"mypy mle_toolbox/.","title":"mypy Type Checking"},{"location":"dev/testing/#black-formatting","text":"black mle_toolbox/. --verbose","title":"black Formatting"},{"location":"dev/testing/#test-coverage","text":"Note : This page and content is still work in progress!","title":"Test Coverage"},{"location":"dev/testing/#unit-tests","text":"pytest -vv tests/unit File loading: tests/unit/test_load_files.py .yaml experiment configuration .json run configuration meta_log.hdf5 hyper_log.pkl Trained models .pkl -based (sklearn) .npy -based (JAX) .pt -based (PyTorch) Experiment launch configuration file generation .qsub .sbash GCP-startup .sh file Logging Individual run assert key errors directory creation - correct file structure are files stored in correct location? is data correctly stored? Merging into meta_log.hdf5 Summary into hyper_log.pkl Reloading correctly All data types supported Tensorboard support Image storage support Experiment Protocol Logging Adding a new experiment Deleting a failed experiment","title":"Unit Tests"},{"location":"dev/testing/#integration-tests","text":"pytest -vv tests/integration Experiment types running on different resources Single configuration: tests/integration/test_single_config.py Multiple configuration: tests/integration/test_multi_configs.py A/Synchronous Grid search experiment: tests/integration/test_grid_search.py Random search experiment: tests/integration/test_random_search.py SMBO search experiment: tests/integration/test_smbo_search.py PBT experiment Report generation Figure generation from meta-log Figure generation from hyper-log .md generation Results retrieval From GCS bucket From remote resource Toolbox initialization Config file changing Encryption ssh credentials GCS integration Pull dummy protocol DB Send results/retrieve results","title":"Integration Tests"},{"location":"mle_toolbox/configuration/","text":"Toolbox Configuration By default the toolbox will run locally and without any GCS bucket backup of your experiment results. Furthermore, a lightweight PickleDB protocol database of your experiments will not be synced with the cloud version. In the following, we walkthrough how to Enable the execution of jobs on remote resources (cluster/storage) from your local machine or from the resource itself. Enable the backing up of your experiment results in a GCS bucket. Enable the backing up of a PickleDB experiment meta log in a GCS bucket. Enable resource monitoring and online dashboard visualization . Enable slack bot notifications for experiment completion and reporting. Note : There are two ways to perform the toolbox configuration: After installation execute mle init . This will walk you through all configuration steps in your CLI and save your configuration in ~/mle_config.toml . Manually edit the config_template.toml template. Move/rename the template to your home directory via mv config_template.toml ~/mle_config.toml . The configuration procedure consists of 3 optional steps, which depend on your needs: Set whether to store all results & your database locally or remote in a GCS bucket. Add SGE and/or Slurm credentials & cluster-specific details (headnode, partitions, proxy server, etc.). Add the GCP project, GCS bucket name and database filename to store your results. Remote Resource Execution The toolbox supports the usage of multiple different compute resources. This includes your local machine, but more importantly remote clusters such as the prominent slurm and grid engine schedulers and Google Cloud Platform VMs. In order to be able to schedule remote jobs from your local machine or retrieve the results from the cluster, you will have to provide your credentials, headnode and partition names as well as some default arguments for jobs: #------------------------------------------------------------------------------# # 2. Configuration for Slurm Cluster #------------------------------------------------------------------------------# [slurm] # Slurm credentials - Only if you want to retrieve results from cluster [slurm.credentials] user_name = '<slurm-user-name>' password = '<slurm-password>' aes_key = '<aes-key>' # Slurm cluster information - Job scheduling, monitoring & retrieval [slurm.info] # Headnode name & partitions to monitor/run jobs on head_names = [ '<headnode1>' ] node_reg_exp = [ '<nodes-to-monitor1>' ] partitions = [ '<partition1>' ] # Info for results retrieval & internet tunneling if local launch main_server_name = '<main-server-ip>' jump_server_name = '<jump-host-ip>' ssh_port = 22 # Add proxy server for internet connection if needed! http_proxy = \"http://<slurm_headnode>:3128/\" https_proxy = \"http://<slurm_headnode>:3128/\" # Default Slurm job arguments (if not supplied in job .yaml config) [slurm.default_job_args] num_logical_cores = 2 partition = '<partition1>' job_name = 'temp' log_file = 'log' err_file = 'err' env_name = '<mle-default-env>' Google Cloud Platform VM Jobs If you want to use the toolbox for orchestrating GCP VMs, you will need to have set up the Google Cloud SDK . This will work as follows: curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-353.0.0-linux-x86_64.tar.gz ./google-cloud-sdk/install.sh ./google-cloud-sdk/bin/gcloud init At initialization you will be required to select a GCP project. Internally the toolbox will call different gcloud commands to launch new VMs and/or monitor the status of running jobs. Google Cloud Storage Backups If you choose so, the toolbox will sync a local version of your experiment protocol database with a GCS bucket. Furthermore, the results of your experiments will be zipped and stored via a unique hash. You can afterwards use mle retrieve in order to retrieve these backed up results from the bucket. These functionalities rely on google-cloud-storage and you having set your GOOGLE_APPLICATION_CREDENTIALS . You have to follow four steps: Create your .json key file here . Name it ~/gcp_mle_key.json Set the path via adding export GOOGLE_APPLICATION_CREDENTIALS = ~/gcp_mle_key.json to your .bashrc or .zshrc file. Create a GCP project and a storage bucket. Have a look here for how to do this. Provide the path, GCP project and bucket to store your results in your ~/mle_config.toml configuration: #------------------------------------------------------------------------------# # 4. GCP Config - Credentials, Project + Buffer for Meta-Experiment Protocol #------------------------------------------------------------------------------# [gcp] # Set absolute path to the GCloud .json credentials - on Slurm/SGE cluster slurm_credentials_path = \"~/<slurm_path_to_gcloud_credentials>.json\" sge_credentials_path = \"~/<sge_path_to_gcloud_credentials>.json\" # Set GCloud project and bucket name for storage/compute instances project_name = \"<gcloud_project_name>\" bucket_name = \"<gcloud_bucket_name>\" PickleDB Experiment Logging We rely on pickleDB for logging meta data of your experiments. It is a lightweight alternative to a full NoSQL-style database, which would require more setup. Instead, pickleDB will create a simple json-style file storing the experiment purpose, id, compute resource, configuration, etc.. You can change the default storage paths for the local and remote version in your ~/mle_config.toml : #------------------------------------------------------------------------------# # 1. General Toolbox Configuration - Verbosity + Whether to use GCS Storage #------------------------------------------------------------------------------# [general] # Local filename to store protocol DB in local_protocol_fname = '~/local_mle_protocol.db' ... #------------------------------------------------------------------------------# # 4. GCP Config - Credentials, Project + Buffer for Meta-Experiment Protocol #------------------------------------------------------------------------------# [gcp] ... # Filename to retrieve from gcloud bucket & where to store protocol_fname = \"gcloud_mle_protocol.db\" Resource Dashboard Monitoring mle monitor - add how to set monitored partition/nodes/etc. Slack Bot Notification If you want to, you can add slack notifications using the slack-clusterbot . If you want to learn more about how to set it up, checkout the wiki documentation . To make it short, you need to create a Bot User OAuth Access Token for your slack workspace and afterwards add this to your mle_config.toml : #------------------------------------------------------------------------------# # 5. Slack Bot Config - OAuth Access Token & Config Path # https://github.com/sprekelerlab/slack-clusterbot/wiki/Installation # OPTIONAL: Only required if you want to slack notifications/updates #------------------------------------------------------------------------------# [slack] # Set authentication token and default username messages are sent to slack_token = \"<xyz-token>\" user_name = \"<user_name>\"","title":"Toolbox Configuration"},{"location":"mle_toolbox/configuration/#toolbox-configuration","text":"By default the toolbox will run locally and without any GCS bucket backup of your experiment results. Furthermore, a lightweight PickleDB protocol database of your experiments will not be synced with the cloud version. In the following, we walkthrough how to Enable the execution of jobs on remote resources (cluster/storage) from your local machine or from the resource itself. Enable the backing up of your experiment results in a GCS bucket. Enable the backing up of a PickleDB experiment meta log in a GCS bucket. Enable resource monitoring and online dashboard visualization . Enable slack bot notifications for experiment completion and reporting. Note : There are two ways to perform the toolbox configuration: After installation execute mle init . This will walk you through all configuration steps in your CLI and save your configuration in ~/mle_config.toml . Manually edit the config_template.toml template. Move/rename the template to your home directory via mv config_template.toml ~/mle_config.toml . The configuration procedure consists of 3 optional steps, which depend on your needs: Set whether to store all results & your database locally or remote in a GCS bucket. Add SGE and/or Slurm credentials & cluster-specific details (headnode, partitions, proxy server, etc.). Add the GCP project, GCS bucket name and database filename to store your results.","title":"Toolbox Configuration"},{"location":"mle_toolbox/configuration/#remote-resource-execution","text":"The toolbox supports the usage of multiple different compute resources. This includes your local machine, but more importantly remote clusters such as the prominent slurm and grid engine schedulers and Google Cloud Platform VMs. In order to be able to schedule remote jobs from your local machine or retrieve the results from the cluster, you will have to provide your credentials, headnode and partition names as well as some default arguments for jobs: #------------------------------------------------------------------------------# # 2. Configuration for Slurm Cluster #------------------------------------------------------------------------------# [slurm] # Slurm credentials - Only if you want to retrieve results from cluster [slurm.credentials] user_name = '<slurm-user-name>' password = '<slurm-password>' aes_key = '<aes-key>' # Slurm cluster information - Job scheduling, monitoring & retrieval [slurm.info] # Headnode name & partitions to monitor/run jobs on head_names = [ '<headnode1>' ] node_reg_exp = [ '<nodes-to-monitor1>' ] partitions = [ '<partition1>' ] # Info for results retrieval & internet tunneling if local launch main_server_name = '<main-server-ip>' jump_server_name = '<jump-host-ip>' ssh_port = 22 # Add proxy server for internet connection if needed! http_proxy = \"http://<slurm_headnode>:3128/\" https_proxy = \"http://<slurm_headnode>:3128/\" # Default Slurm job arguments (if not supplied in job .yaml config) [slurm.default_job_args] num_logical_cores = 2 partition = '<partition1>' job_name = 'temp' log_file = 'log' err_file = 'err' env_name = '<mle-default-env>'","title":"Remote Resource Execution"},{"location":"mle_toolbox/configuration/#google-cloud-platform-vm-jobs","text":"If you want to use the toolbox for orchestrating GCP VMs, you will need to have set up the Google Cloud SDK . This will work as follows: curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-353.0.0-linux-x86_64.tar.gz ./google-cloud-sdk/install.sh ./google-cloud-sdk/bin/gcloud init At initialization you will be required to select a GCP project. Internally the toolbox will call different gcloud commands to launch new VMs and/or monitor the status of running jobs.","title":"Google Cloud Platform VM Jobs"},{"location":"mle_toolbox/configuration/#google-cloud-storage-backups","text":"If you choose so, the toolbox will sync a local version of your experiment protocol database with a GCS bucket. Furthermore, the results of your experiments will be zipped and stored via a unique hash. You can afterwards use mle retrieve in order to retrieve these backed up results from the bucket. These functionalities rely on google-cloud-storage and you having set your GOOGLE_APPLICATION_CREDENTIALS . You have to follow four steps: Create your .json key file here . Name it ~/gcp_mle_key.json Set the path via adding export GOOGLE_APPLICATION_CREDENTIALS = ~/gcp_mle_key.json to your .bashrc or .zshrc file. Create a GCP project and a storage bucket. Have a look here for how to do this. Provide the path, GCP project and bucket to store your results in your ~/mle_config.toml configuration: #------------------------------------------------------------------------------# # 4. GCP Config - Credentials, Project + Buffer for Meta-Experiment Protocol #------------------------------------------------------------------------------# [gcp] # Set absolute path to the GCloud .json credentials - on Slurm/SGE cluster slurm_credentials_path = \"~/<slurm_path_to_gcloud_credentials>.json\" sge_credentials_path = \"~/<sge_path_to_gcloud_credentials>.json\" # Set GCloud project and bucket name for storage/compute instances project_name = \"<gcloud_project_name>\" bucket_name = \"<gcloud_bucket_name>\"","title":"Google Cloud Storage Backups"},{"location":"mle_toolbox/configuration/#pickledb-experiment-logging","text":"We rely on pickleDB for logging meta data of your experiments. It is a lightweight alternative to a full NoSQL-style database, which would require more setup. Instead, pickleDB will create a simple json-style file storing the experiment purpose, id, compute resource, configuration, etc.. You can change the default storage paths for the local and remote version in your ~/mle_config.toml : #------------------------------------------------------------------------------# # 1. General Toolbox Configuration - Verbosity + Whether to use GCS Storage #------------------------------------------------------------------------------# [general] # Local filename to store protocol DB in local_protocol_fname = '~/local_mle_protocol.db' ... #------------------------------------------------------------------------------# # 4. GCP Config - Credentials, Project + Buffer for Meta-Experiment Protocol #------------------------------------------------------------------------------# [gcp] ... # Filename to retrieve from gcloud bucket & where to store protocol_fname = \"gcloud_mle_protocol.db\"","title":"PickleDB Experiment Logging"},{"location":"mle_toolbox/configuration/#resource-dashboard-monitoring","text":"mle monitor - add how to set monitored partition/nodes/etc.","title":"Resource Dashboard Monitoring"},{"location":"mle_toolbox/configuration/#slack-bot-notification","text":"If you want to, you can add slack notifications using the slack-clusterbot . If you want to learn more about how to set it up, checkout the wiki documentation . To make it short, you need to create a Bot User OAuth Access Token for your slack workspace and afterwards add this to your mle_config.toml : #------------------------------------------------------------------------------# # 5. Slack Bot Config - OAuth Access Token & Config Path # https://github.com/sprekelerlab/slack-clusterbot/wiki/Installation # OPTIONAL: Only required if you want to slack notifications/updates #------------------------------------------------------------------------------# [slack] # Set authentication token and default username messages are sent to slack_token = \"<xyz-token>\" user_name = \"<user_name>\"","title":"Slack Bot Notification"},{"location":"mle_toolbox/experiments/","text":"Supported Experiments Note : This page and content is still work in progress! Pre-/Post-Processing # Parameters for the pre-processing job pre_processing_args : processing_fname : \"<run_preprocessing>.py\" processing_job_args : num_logical_cores : 2 time_per_job : \"00:01:00\" extra_cmd_line_input : figures_dir : \"experiments/data\" # Parameters for the post-processing job post_processing_args : processing_fname : \"<run_postprocessing>.py\" processing_job_args : num_logical_cores : 2 time_per_job : \"00:01:00\" extra_cmd_line_input : figures_dir : \"experiments/figures\" Multiple Configurations & Seeds multi_config_args : config_fnames : - \"config_1.json\" - \"config_2.json\" num_seeds : 2 Hyperparameter Search Given a fixed training configuration file and the Python training script the only thing that has to be adapted for the different types of experiments is the meta configuration file. # Parameters specific to the hyperparameter search param_search_args : search_logging : reload_log : False verbose_log : True max_objective : True problem_type : \"final\" eval_metrics : \"test_loss\" search_resources : num_search_batches : 2 num_evals_per_batch : 2 num_seeds_per_eval : 1 search_config : search_type : \"grid\" # \"random\"/\"smbo\" search_schedule : \"sync\" search_params : categorical : opt_type : - \"Adam\" - \"RMSprop\" real : l_rate : begin : 1e-5 end : 1e-2 bins : 2 Population-Based Training # Parameters specific to the population-based training pbt_args : pbt_logging : max_objective : False eval_metric : \"test_loss\" pbt_resources : num_population_members : 10 num_total_update_steps : 2000 num_steps_until_ready : 500 num_steps_until_eval : 100 pbt_config : pbt_params : real : l_rate : begin : 1e-5 end : 1e-2 exploration : strategy : \"perturb\" selection : strategy : \"truncation\"","title":"Supported Experiments"},{"location":"mle_toolbox/experiments/#supported-experiments","text":"Note : This page and content is still work in progress!","title":"Supported Experiments"},{"location":"mle_toolbox/experiments/#pre-post-processing","text":"# Parameters for the pre-processing job pre_processing_args : processing_fname : \"<run_preprocessing>.py\" processing_job_args : num_logical_cores : 2 time_per_job : \"00:01:00\" extra_cmd_line_input : figures_dir : \"experiments/data\" # Parameters for the post-processing job post_processing_args : processing_fname : \"<run_postprocessing>.py\" processing_job_args : num_logical_cores : 2 time_per_job : \"00:01:00\" extra_cmd_line_input : figures_dir : \"experiments/figures\"","title":"Pre-/Post-Processing"},{"location":"mle_toolbox/experiments/#multiple-configurations-seeds","text":"multi_config_args : config_fnames : - \"config_1.json\" - \"config_2.json\" num_seeds : 2","title":"Multiple Configurations &amp; Seeds"},{"location":"mle_toolbox/experiments/#hyperparameter-search","text":"Given a fixed training configuration file and the Python training script the only thing that has to be adapted for the different types of experiments is the meta configuration file. # Parameters specific to the hyperparameter search param_search_args : search_logging : reload_log : False verbose_log : True max_objective : True problem_type : \"final\" eval_metrics : \"test_loss\" search_resources : num_search_batches : 2 num_evals_per_batch : 2 num_seeds_per_eval : 1 search_config : search_type : \"grid\" # \"random\"/\"smbo\" search_schedule : \"sync\" search_params : categorical : opt_type : - \"Adam\" - \"RMSprop\" real : l_rate : begin : 1e-5 end : 1e-2 bins : 2","title":"Hyperparameter Search"},{"location":"mle_toolbox/experiments/#population-based-training","text":"# Parameters specific to the population-based training pbt_args : pbt_logging : max_objective : False eval_metric : \"test_loss\" pbt_resources : num_population_members : 10 num_total_update_steps : 2000 num_steps_until_ready : 500 num_steps_until_eval : 100 pbt_config : pbt_params : real : l_rate : begin : 1e-5 end : 1e-2 exploration : strategy : \"perturb\" selection : strategy : \"truncation\"","title":"Population-Based Training"},{"location":"mle_toolbox/getting_started/","text":"Video Tutorials Note : I gave a general overview talk on the setup and usage of the toolbox: TODO : You can watch the talk here . You can have a look at the slide deck here . 1. Motivation and Use-Case 2. Installation & Credentials Setup 3. Different mle <subcommand> mle run : Different experiment types mle retrieve : Getting your results (local or GCP) mle report : Generating .md / .html reports mle monitor : On different resources 4. Running Jobs on Gloud VMs Running Jobs with GCP and GCS storage 5. Analyzing Experiment Results Manual inspection of hyper_log and meta_log . 6. Hypothesis Testing","title":"Video Tutorials"},{"location":"mle_toolbox/getting_started/#video-tutorials","text":"Note : I gave a general overview talk on the setup and usage of the toolbox: TODO : You can watch the talk here . You can have a look at the slide deck here .","title":"Video Tutorials"},{"location":"mle_toolbox/getting_started/#1-motivation-and-use-case","text":"","title":"1. Motivation and Use-Case"},{"location":"mle_toolbox/getting_started/#2-installation-credentials-setup","text":"","title":"2. Installation &amp; Credentials Setup"},{"location":"mle_toolbox/getting_started/#3-different-mle-subcommand","text":"mle run : Different experiment types mle retrieve : Getting your results (local or GCP) mle report : Generating .md / .html reports mle monitor : On different resources","title":"3. Different mle &lt;subcommand&gt;"},{"location":"mle_toolbox/getting_started/#4-running-jobs-on-gloud-vms","text":"Running Jobs with GCP and GCS storage","title":"4. Running Jobs on Gloud VMs"},{"location":"mle_toolbox/getting_started/#5-analyzing-experiment-results","text":"Manual inspection of hyper_log and meta_log .","title":"5. Analyzing Experiment Results"},{"location":"mle_toolbox/getting_started/#6-hypothesis-testing","text":"","title":"6. Hypothesis Testing"},{"location":"mle_toolbox/subcommands/","text":"MLE Subcommands mle run - Experiment Execution An experiment can be launched from the command line via: mle run <experiment_config>.yaml You can add several command line options, which can come in handy when debugging or if you want to launch multiple experiments sequentially: --no_welcome : Don't print welcome messages at experiment launch. --no_protocol : Do not record experiment in the PickleDB protocol database. --resource_to_run <resource> : Run the experiment on the specified resource. In the following we walk through the different types of supported experiments and show how to provide the necessary configuration specifications. mle monitor - Monitor Resources mle retrieve - Retrieve Results mle report - Report Results mle init - Setup The Toolbox Configuration mle sync-gcs - Download GCS-Backed Experiments mle project - Initialize New Template Project mle protocol - List State of Protocol","title":"MLE Subcommands"},{"location":"mle_toolbox/subcommands/#mle-subcommands","text":"","title":"MLE Subcommands"},{"location":"mle_toolbox/subcommands/#mle-run-experiment-execution","text":"An experiment can be launched from the command line via: mle run <experiment_config>.yaml You can add several command line options, which can come in handy when debugging or if you want to launch multiple experiments sequentially: --no_welcome : Don't print welcome messages at experiment launch. --no_protocol : Do not record experiment in the PickleDB protocol database. --resource_to_run <resource> : Run the experiment on the specified resource. In the following we walk through the different types of supported experiments and show how to provide the necessary configuration specifications.","title":"mle run - Experiment Execution"},{"location":"mle_toolbox/subcommands/#mle-monitor-monitor-resources","text":"","title":"mle monitor - Monitor Resources"},{"location":"mle_toolbox/subcommands/#mle-retrieve-retrieve-results","text":"","title":"mle retrieve - Retrieve Results"},{"location":"mle_toolbox/subcommands/#mle-report-report-results","text":"","title":"mle report - Report Results"},{"location":"mle_toolbox/subcommands/#mle-init-setup-the-toolbox-configuration","text":"","title":"mle init - Setup The Toolbox Configuration"},{"location":"mle_toolbox/subcommands/#mle-sync-gcs-download-gcs-backed-experiments","text":"","title":"mle sync-gcs - Download GCS-Backed Experiments"},{"location":"mle_toolbox/subcommands/#mle-project-initialize-new-template-project","text":"","title":"mle project - Initialize New Template Project"},{"location":"mle_toolbox/subcommands/#mle-protocol-list-state-of-protocol","text":"","title":"mle protocol - List State of Protocol"},{"location":"mle_toolbox/toolbox/","text":"MLE-Toolbox Overview ML researchers need to coordinate different types of experiments on separate remote resources. The Machine Learning Experiment (MLE)-Toolbox is designed to facilitate the workflow by providing a simple interface, standardized logging, many common ML experiment types (multi-seed/configurations, grid-searches and hyperparameter optimization pipelines). You can run experiments on your local machine, high-performance compute clusters ( Slurm and Sun Grid Engine ) as well as on cloud VMs ( GCP ). The results are archived (locally/ GCS bucket ) and can easily be retrieved or automatically summarized/reported. What Does The mle-toolbox Provide? \ud83e\uddd1\u200d\ud83d\udd27 API for launching jobs on cluster/cloud computing platforms (Slurm, GridEngine, GCP). Common machine learning research experiment setups: Launching and collecting multiple random seeds in parallel/batches or async. Hyperparameter searches: Random, Grid, SMBO, PBT and Nevergrad. Pre- and post-processing pipelines for data preparation/result visualization. Automated report generation for hyperparameter search experiments. Storage/retrieval of results and database in Google Cloud Storage Bucket. Resource monitoring with dashboard visualization. The 4 Step mle-toolbox Cooking Recipe \ud83c\udf72 Follow the instructions below to install the mle-toolbox and set up your credentials/configuration. Read the docs explaining the pillars of the toolbox & the experiment meta-configuration job .yaml files . Learn more about the individual infrastructure subpackages with the dedicated tutorial . Check out the examples \ud83d\udcc4 to get started: Single Objective Optimization , Multi Objective Optimization . Run your own experiments using the template files, project and mle run . Installation \u23f3 If you want to use the toolbox on your local machine follow the instructions locally. Otherwise do so on your respective cluster resource (Slurm/SGE). A PyPI installation is available via: pip install mle-toolbox Alternatively, you can clone this repository and afterwards 'manually' install it: git clone https://github.com/mle-infrastructure/mle-toolbox.git cd mle-toolbox pip install -e . Setting Up Your Toolbox Configuration \ud83e\uddd1\u200d\ud83c\udfa8 By default the toolbox will support local runs without any GCS storage of your experiments. If you want to integrate the mle-toolbox with your SGE/Slurm clusters, you have to provide additional data. There 2 ways to do so: After installation type mle init . This will walk you through all configuration steps in your CLI and save your configuration in ~/mle_config.toml . Manually edit the config_template.toml template. Move/rename the template to your home directory via mv config_template.toml ~/mle_config.toml . The configuration procedure consists of 4 optional steps, which depend on your needs: Set whether to store all results & your database locally or remote in a GCS bucket. Add SGE and/or Slurm credentials & cluster-specific details (headnode, partitions, proxy server, etc.). Add the GCP project, GCS bucket name and database filename to store your results. Add credentials for a slack bot integration that notifies you about the state of your experiments. The Core Toolbox Subcommands \ud83c\udf31 You are now ready to dive deeper into the specifics of experiment configuration and can start running your first experiments from the cluster (or locally on your machine) with the following commands: Command Description \ud83d\ude80 mle run Start up an experiment (multi-config/seeds, search). \ud83d\udda5\ufe0f mle monitor Monitor resource utilisation ( mle-monitor wrapper). \ud83d\udce5 mle retrieve Retrieve experiment result from GCS/cluster. \ud83d\udc8c mle report Create an experiment report with figures. \u23f3 mle init Setup of credentials & toolbox settings. \ud83d\udd04 mle sync Extract all GCS-stored results to your local drive. \ud83d\uddc2 mle project Initialize a new project by cloning mle-project . \ud83d\udcdd mle protocol List a summary of the most recent experiments. You can find more documentation for each subcommand here .","title":"MLE-Toolbox Overview"},{"location":"mle_toolbox/toolbox/#mle-toolbox-overview","text":"ML researchers need to coordinate different types of experiments on separate remote resources. The Machine Learning Experiment (MLE)-Toolbox is designed to facilitate the workflow by providing a simple interface, standardized logging, many common ML experiment types (multi-seed/configurations, grid-searches and hyperparameter optimization pipelines). You can run experiments on your local machine, high-performance compute clusters ( Slurm and Sun Grid Engine ) as well as on cloud VMs ( GCP ). The results are archived (locally/ GCS bucket ) and can easily be retrieved or automatically summarized/reported.","title":"MLE-Toolbox Overview"},{"location":"mle_toolbox/toolbox/#what-does-the-mle-toolbox-provide","text":"API for launching jobs on cluster/cloud computing platforms (Slurm, GridEngine, GCP). Common machine learning research experiment setups: Launching and collecting multiple random seeds in parallel/batches or async. Hyperparameter searches: Random, Grid, SMBO, PBT and Nevergrad. Pre- and post-processing pipelines for data preparation/result visualization. Automated report generation for hyperparameter search experiments. Storage/retrieval of results and database in Google Cloud Storage Bucket. Resource monitoring with dashboard visualization.","title":"What Does The mle-toolbox Provide? \ud83e\uddd1\u200d\ud83d\udd27"},{"location":"mle_toolbox/toolbox/#the-4-step-mle-toolbox-cooking-recipe","text":"Follow the instructions below to install the mle-toolbox and set up your credentials/configuration. Read the docs explaining the pillars of the toolbox & the experiment meta-configuration job .yaml files . Learn more about the individual infrastructure subpackages with the dedicated tutorial . Check out the examples \ud83d\udcc4 to get started: Single Objective Optimization , Multi Objective Optimization . Run your own experiments using the template files, project and mle run .","title":"The 4 Step mle-toolbox Cooking Recipe \ud83c\udf72"},{"location":"mle_toolbox/toolbox/#installation","text":"If you want to use the toolbox on your local machine follow the instructions locally. Otherwise do so on your respective cluster resource (Slurm/SGE). A PyPI installation is available via: pip install mle-toolbox Alternatively, you can clone this repository and afterwards 'manually' install it: git clone https://github.com/mle-infrastructure/mle-toolbox.git cd mle-toolbox pip install -e .","title":"Installation \u23f3"},{"location":"mle_toolbox/toolbox/#setting-up-your-toolbox-configuration","text":"By default the toolbox will support local runs without any GCS storage of your experiments. If you want to integrate the mle-toolbox with your SGE/Slurm clusters, you have to provide additional data. There 2 ways to do so: After installation type mle init . This will walk you through all configuration steps in your CLI and save your configuration in ~/mle_config.toml . Manually edit the config_template.toml template. Move/rename the template to your home directory via mv config_template.toml ~/mle_config.toml . The configuration procedure consists of 4 optional steps, which depend on your needs: Set whether to store all results & your database locally or remote in a GCS bucket. Add SGE and/or Slurm credentials & cluster-specific details (headnode, partitions, proxy server, etc.). Add the GCP project, GCS bucket name and database filename to store your results. Add credentials for a slack bot integration that notifies you about the state of your experiments.","title":"Setting Up Your Toolbox Configuration \ud83e\uddd1\u200d\ud83c\udfa8"},{"location":"mle_toolbox/toolbox/#the-core-toolbox-subcommands","text":"You are now ready to dive deeper into the specifics of experiment configuration and can start running your first experiments from the cluster (or locally on your machine) with the following commands: Command Description \ud83d\ude80 mle run Start up an experiment (multi-config/seeds, search). \ud83d\udda5\ufe0f mle monitor Monitor resource utilisation ( mle-monitor wrapper). \ud83d\udce5 mle retrieve Retrieve experiment result from GCS/cluster. \ud83d\udc8c mle report Create an experiment report with figures. \u23f3 mle init Setup of credentials & toolbox settings. \ud83d\udd04 mle sync Extract all GCS-stored results to your local drive. \ud83d\uddc2 mle project Initialize a new project by cloning mle-project . \ud83d\udcdd mle protocol List a summary of the most recent experiments. You can find more documentation for each subcommand here .","title":"The Core Toolbox Subcommands \ud83c\udf31"}]}